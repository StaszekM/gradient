{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/Users/jjaniak/Documents/studia/projekt/gradient')\n",
    "\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "import numpy as np\n",
    "\n",
    "from shapely.wkt import loads\n",
    "\n",
    "from esda.moran import Moran_Local\n",
    "import libpysal\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from src.embedders.osm_data_embedder import OSMDataEmbedder\n",
    "from srai.regionalizers import geocode_to_region_gdf\n",
    "from srai.embedders import CountEmbedder\n",
    "from srai.regionalizers import H3Regionalizer\n",
    "from srai.loaders.osm_loaders.filters import OsmTagsFilter\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "city_name = \"Pozna\\u0144\"\n",
    "nominatim_city_name = \"Pozna\\u0144, Poland\"\n",
    "year = 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query: OsmTagsFilter = {\"highway\": True, \"railway\": True, \"route\": True}\n",
    "\n",
    "\n",
    "def create_hex_gds(h3_resolution, city_name=city_name):\n",
    "    data_embedder = OSMDataEmbedder(\n",
    "        area=geocode_to_region_gdf(nominatim_city_name),\n",
    "        embedder=CountEmbedder(),\n",
    "        regionalizer=H3Regionalizer(resolution=h3_resolution),\n",
    "        query=query,\n",
    "    )\n",
    "\n",
    "    filename = f\"data/baseline-datasets/in/{city_name}-hex-res-{h3_resolution}-and-features-gdf.shp\"\n",
    "\n",
    "    if not os.path.exists(filename):\n",
    "        hex_and_features_gdf = data_embedder.make_embeddings()  # type: ignore\n",
    "        hex_and_features_gdf.to_file(\n",
    "            filename,\n",
    "            index=True,\n",
    "        )\n",
    "    else:\n",
    "        hex_and_features_gdf = gpd.read_file(filename)\n",
    "        hex_and_features_gdf.set_index(\"region_id\", inplace=True)\n",
    "\n",
    "    return hex_and_features_gdf\n",
    "\n",
    "\n",
    "def get_accidents_gdf(h3_resolution, city_name=city_name, year=year):\n",
    "    filename = f\"data/accidents_in_hex/{city_name}_accidents_{year}_res{h3_resolution}.csv\"\n",
    "    if not os.path.exists(filename):\n",
    "        raise FileNotFoundError(f\"The file {filename} does not exist.\")\n",
    "    else:\n",
    "        accidents_df = pd.read_csv(filename)\n",
    "        accidents_df['geometry'] = accidents_df['geometry'].apply(loads)\n",
    "        accidents_gdf = gpd.GeoDataFrame(accidents_df, geometry='geometry', crs=\"EPSG:4326\")\n",
    "        return accidents_gdf\n",
    "    \n",
    "def merge_gdf(accidents_gdf, hex_and_features_gdf):\n",
    "    merged_gdf = gpd.sjoin(left_df=accidents_gdf, right_df=hex_and_features_gdf, how='inner', op='intersects')\n",
    "    merged_gdf = merged_gdf.drop(columns='index_right')\n",
    "    merged_gdf.rename(columns={'count': 'num_accidents'}, inplace=True)\n",
    "    merged_gdf['num_accidents'] = merged_gdf['num_accidents'].astype(int)\n",
    "\n",
    "    return merged_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_moran(df, column):\n",
    "    w = libpysal.weights.Queen.from_dataframe(df)\n",
    "    y = df[column].values\n",
    "    moran_loc = Moran_Local(y, w)\n",
    "    return moran_loc.Is, moran_loc.p_sim, moran_loc.q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_analysis(merged_gdf):\n",
    "    all_features = merged_gdf.drop(columns=['geometry', 'region_id', 'num_accidents', 'binary_accidents']).columns.to_list()\n",
    "    \n",
    "    correlation_matrix = merged_gdf[all_features + ['binary_accidents']].corr()\n",
    "    correlation_values = correlation_matrix['binary_accidents']\n",
    "\n",
    "    correlation_metrics = correlation_values.drop('binary_accidents').describe()\n",
    "\n",
    "    # Calculate the interquartile range (IQR)\n",
    "    Q1 = correlation_values.drop('binary_accidents').quantile(0.25)\n",
    "    Q3 = correlation_values.drop('binary_accidents').quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    # Filter features based on IQR\n",
    "    best_features = correlation_values[(correlation_values < (Q1 - 0.5 * IQR)) | (correlation_values > (Q3 + 0.5 * IQR))].index.to_list()\n",
    "\n",
    "    best_features.remove('binary_accidents')\n",
    "    correlation_matrix_best_features = merged_gdf[best_features + ['binary_accidents']].corr()\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(correlation_matrix_best_features, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "    plt.title('Correlation Heatmap (best features)')\n",
    "    plt.show()\n",
    "\n",
    "    return best_features, correlation_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shap_analysis(h3_resolution, merged_gdf, selected_features, city_name=city_name, save_force_plot=False):\n",
    "    y = merged_gdf['binary_accidents']\n",
    "    X = merged_gdf[selected_features]\n",
    "\n",
    "    X_train, X_test, y_train, _ = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    linear_lr = LogisticRegression(max_iter=1000)\n",
    "    linear_lr.fit(X_train, y_train)\n",
    "\n",
    "    #Przygotowanie przybliżonej próbki tła za pomocą metody k-means.\n",
    "    background_summary = shap.kmeans(X_train, 10)\n",
    "\n",
    "    explainer = shap.KernelExplainer(linear_lr.predict_proba, background_summary)\n",
    "\n",
    "    #Obliczenie SHAP wartości dla danych testowych (X_test), co pozwala na zrozumienie, jak każda zmienna przyczynia się do przewidywanej wartości.\n",
    "    shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "    shap.summary_plot(shap_values, X_test)\n",
    "\n",
    "    if save_force_plot:\n",
    "        shap.initjs()\n",
    "        p = shap.force_plot(explainer.expected_value[0], shap_values[0], X_test)\n",
    "        shap.save_html(f'{city_name}_res_{h3_resolution}_force_plot_best_features.html', p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set resolutions\n",
    "resolutions = [8, 9, 10]\n",
    "\n",
    "# Initialize dataframe and dictionaries to store results\n",
    "distribution_results = pd.DataFrame(columns=['Resolution', 'Num_Hexes', 'Accidents_Mean', 'Accidents_Std','Binary_Accidents_Mean', 'Binary_Accidents_Std'])\n",
    "moran_i_results = pd.DataFrame(columns=['Resolution', 'Mean', 'Std', 'Min','Max'])\n",
    "df_correlation_metrics = pd.DataFrame()\n",
    "\n",
    "best_features_based_on_correlation = {}\n",
    "local_moran_results = {} \n",
    "\n",
    "# Loop through resolutions\n",
    "for resolution in resolutions:\n",
    "    \n",
    "    print(f\"Analysis for Resolution = {resolution}\")\n",
    "    \n",
    "    # Create hex dataframes\n",
    "    hex_and_features_gdf = create_hex_gds(h3_resolution=resolution, city_name=city_name)\n",
    "    accidents_gdf = get_accidents_gdf(h3_resolution=resolution, city_name=city_name, year=year)\n",
    "    merged_gdf = merge_gdf(accidents_gdf, hex_and_features_gdf)\n",
    "    \n",
    "    # Convert accidents count to binary classification\n",
    "    merged_gdf['binary_accidents'] = 0  # Initialize with 0\n",
    "    merged_gdf.loc[merged_gdf[\"num_accidents\"] > 0, \"binary_accidents\"] = 1.0\n",
    "\n",
    "    new_row = {\n",
    "        'Resolution': resolution,\n",
    "        'Num_Hexes': len(merged_gdf),\n",
    "        'Accidents_Mean': merged_gdf['num_accidents'].mean(),\n",
    "        'Accidents_Std': merged_gdf['num_accidents'].std(),\n",
    "        'Binary_Accidents_Mean': merged_gdf['binary_accidents'].mean(),\n",
    "        'Binary_Accidents_Std': merged_gdf['binary_accidents'].std()\n",
    "    }\n",
    "    result_df = pd.DataFrame([new_row], columns=distribution_results.columns)\n",
    "    distribution_results = pd.concat([distribution_results, result_df], ignore_index=True)\n",
    "    \n",
    "    # Summary statistics of numerical columns\n",
    "    summary_statistics = merged_gdf.describe()\n",
    "    mean_row = summary_statistics.loc['mean']\n",
    "    sorted_columns = mean_row.sort_values().index\n",
    "    summary_statistics_sorted = summary_statistics[sorted_columns]\n",
    "    print(\"Summary Statistics of Numerical Columns (Sorted by Mean):\")\n",
    "    display(summary_statistics_sorted)\n",
    "    \n",
    "    # Correlation\n",
    "    best_features, correlation_metrics = correlation_analysis(merged_gdf)\n",
    "    df_correlation_metrics = pd.concat([df_correlation_metrics, correlation_metrics], axis=1)\n",
    "    best_features_based_on_correlation[resolution] = best_features\n",
    "    \n",
    "    shap_analysis(h3_resolution=resolution, merged_gdf=merged_gdf, selected_features=best_features)\n",
    "    \n",
    "    # Calculate Local Moran's I\n",
    "    moran_i, p_sim, q = local_moran(merged_gdf, column='num_accidents')\n",
    "    local_moran_results[resolution] = {'moran_i': moran_i, 'p_sim': p_sim, 'q': q}\n",
    "    new_row = {\n",
    "        'Resolution': resolution,\n",
    "        'Mean':  np.mean(moran_i),\n",
    "        'Std': np.std(moran_i),\n",
    "        'Min': np.min(moran_i),\n",
    "        'Max': np.min(moran_i),\n",
    "    }\n",
    "    result_df = pd.DataFrame([new_row], columns=distribution_results.columns)\n",
    "    distribution_results = pd.concat([distribution_results, result_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_correlation_metrics.columns = [f'resolution={resolution}' for resolution in resolutions]\n",
    "print(\"Correlation Metrics of Binary Accidents with All Features:\")\n",
    "display(df_correlation_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rozkład wypadków"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Distribution of Accidents in Hexes:\")\n",
    "display(distribution_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Istotne cechy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best Features Based on Correlation:\\n\")\n",
    "for resolution, best_features in best_features_based_on_correlation.items():\n",
    "    print(f\"\\nResolution {resolution}:\")\n",
    "    for feature in best_features:\n",
    "        print(f\"{feature}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_resolution_8 = best_features_based_on_correlation[8]\n",
    "features_resolution_9 = best_features_based_on_correlation[9]\n",
    "features_resolution_10 = best_features_based_on_correlation[10]\n",
    "\n",
    "# Find common features\n",
    "common_features = set(features_resolution_8) & set(features_resolution_9) & set(features_resolution_10)\n",
    "\n",
    "print(\"Common Features Across All Resolutions:\")\n",
    "for feature in common_features:\n",
    "    print(feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moran's I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Local Moran's I Results:\")\n",
    "for resolution, results in local_moran_results.items():\n",
    "    print(f\"Resolution {resolution}: Moran's I = {results['moran_i']}, p_sim = {results['p_sim']}, q = {results['q']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {}\n",
    "for resolution, results in local_moran_results.items():\n",
    "    res[resolution] = []\n",
    "    for key, value in results.items():\n",
    "        res[resolution].extend([np.mean(value), np.std(value), np.min(value), np.max(value)])\n",
    "\n",
    "local_moran_metrics = list(local_moran_results.values())[0].keys()\n",
    "id_names = [f'{key}_{stat}' for key in local_moran_metrics for stat in ['mean', 'std', 'min', 'max']]\n",
    "        \n",
    "df = pd.DataFrame(res, index=id_names)\n",
    "df.columns = [f'resolution={resolution}' for resolution in df.columns]\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "studia_projekt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
