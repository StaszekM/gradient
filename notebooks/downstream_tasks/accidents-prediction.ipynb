{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54c24ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "notebook_path = os.path.abspath(\"__file__\")\n",
    "notebook_directory = os.path.dirname(notebook_path)\n",
    "parent_directory = os.path.dirname(notebook_directory)\n",
    "\n",
    "parent_parent_directory = os.path.dirname(parent_directory)\n",
    "\n",
    "sys.path.append(parent_parent_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d440116b8deaa04b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T05:54:43.462692Z",
     "start_time": "2024-04-29T05:54:40.712864Z"
    }
   },
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from src.organized_datasets_creation.utils import resolve_nominatim_city_name\n",
    "from src.graph_layering.create_dataframes import create_osmnx_dataframes\n",
    "from src.organized_datasets_creation.utils import convert_nominatim_name_to_filename\n",
    "from src.graph_layering.graph_layer_creator import GraphLayerController\n",
    "import pandas as pd\n",
    "from typing import cast\n",
    "import os\n",
    "from src.graph_layering.graph_layer_creator import SourceType\n",
    "import warnings\n",
    "from src.graph_layering.create_hetero_data import create_hetero_data\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from tqdm import tqdm\n",
    "# import wandb.util\n",
    "# import wandb\n",
    "import os\n",
    "\n",
    "import glob\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21b83fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WANDB_API_KEY = os.environ.get(\"WANDB_API_KEY\", None)\n",
    "# assert (\n",
    "#     WANDB_API_KEY is not None\n",
    "# ), \"WANDB_API_KEY is not set, did you forget it in the config file?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2e7d25fb99b34b10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T05:54:43.480822Z",
     "start_time": "2024-04-29T05:54:43.463793Z"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# general settings\n",
    "# ORGANIZED_DATASETS_LOCATION = (\n",
    "#     \"/home/staszek/mgr/gradient/gradient/data/organized-datasets\"\n",
    "# )\n",
    "ORGANIZED_DATASETS_LOCATION = (\n",
    "    \"C:/Users/Natalia/Desktop/gradient_new/gradient/data/organized-datasets\"\n",
    ")\n",
    "\n",
    "# downstream task settings\n",
    "# ACCIDENTS_LOCATION = \"/home/staszek/mgr/gradient/gradient/data/wypadki-pl/accidents.csv\"\n",
    "ACCIDENTS_LOCATION = \"C:/Users/Natalia/Desktop/gradient_new/gradient/data/wypadki-pl/accidents.csv\"\n",
    "# TRAIN_SAVE_DIR = \"/media/staszek/m2-mint/gradient_logs/\"\n",
    "\n",
    "SWEEP_RUNS_COUNT = 50\n",
    "EPOCHS = 300\n",
    "\n",
    "H3_RESOLUTION = 9\n",
    "H3_YEAR = 2017\n",
    "H3_EMBEDDING_METHOD = \"count-embedder\"\n",
    "\n",
    "DATASET_CONTRUCTION_METHOD = \"drop-non-matching-columns\"\n",
    "\n",
    "WANDB_SWEEP_PARAMS = {\n",
    "    \"method\": \"bayes\",\n",
    "    \"metric\": {\"name\": \"mean_f1\", \"goal\": \"maximize\"},\n",
    "    \"parameters\": {\n",
    "        \"hidden_channels\": {\"values\": [10, 20, 30, 40, 50]},\n",
    "        \"learning_rate\": {\n",
    "            \"distribution\": \"log_uniform_values\",\n",
    "            \"min\": 1e-5,\n",
    "            \"max\": 1e-2,\n",
    "        },\n",
    "        \"num_conv_layers\": {\"values\": [1, 2, 3, 4, 5]},\n",
    "        \"lin_layer_size\": {\"values\": [8, 16, 32, 64, 128]},\n",
    "        \"num_lin_layers\": {\"values\": [0, 1, 2, 3, 4]},\n",
    "        \"weight_decay\": {\n",
    "            \"distribution\": \"log_uniform_values\",\n",
    "            \"min\": 1e-5,\n",
    "            \"max\": 1e-2,\n",
    "        },\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6aabe258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available h3 embedding methods:\n",
      "['count-embedder', 'hex2vec', 'highway2vec']\n"
     ]
    }
   ],
   "source": [
    "available_h3_embedding_methods = glob.glob(ORGANIZED_DATASETS_LOCATION + \"/**/**/**/*\")\n",
    "available_h3_embedding_methods = [\n",
    "    os.path.basename(x) for x in available_h3_embedding_methods\n",
    "]\n",
    "available_h3_embedding_methods = np.unique(available_h3_embedding_methods).tolist()\n",
    "print(\"Available h3 embedding methods:\")\n",
    "assert H3_EMBEDDING_METHOD in available_h3_embedding_methods, (\n",
    "    f\"Chosen h3 embedding method is not available. \"\n",
    "    f\"Available methods are: {available_h3_embedding_methods}\"\n",
    ")\n",
    "print(available_h3_embedding_methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1426d7ddde7af4b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T05:54:44.922146Z",
     "start_time": "2024-04-29T05:54:43.481459Z"
    }
   },
   "outputs": [],
   "source": [
    "accidents = gpd.read_file(ACCIDENTS_LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa453cc94b7c9a03",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T05:54:44.938471Z",
     "start_time": "2024-04-29T05:54:44.922907Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cities:\n",
      "['WrocÅ‚aw, Poland', 'Szczecin, Poland', 'PoznaÅ„, Poland', 'KrakÃ³w, Poland', 'Warszawa, Poland']\n"
     ]
    }
   ],
   "source": [
    "cities = list(map(lambda x: x + \", Poland\", accidents[\"mie_nazwa\"].unique()))\n",
    "print(\"Cities:\")\n",
    "print(cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5583ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cities:\n",
      "['Wrocław, Poland', 'Szczecin, Poland', 'Poznań, Poland', 'Kraków, Poland', 'Warszawa, Poland']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "# Read the file using pandas with the correct encoding\n",
    "accidents = pd.read_csv(ACCIDENTS_LOCATION, encoding='utf-8')\n",
    "\n",
    "# Convert the DataFrame to a GeoDataFrame if necessary\n",
    "# Assuming 'wsp_gps_x' and 'wsp_gps_y' are the longitude and latitude\n",
    "geometry = gpd.points_from_xy(accidents.wsp_gps_x, accidents.wsp_gps_y)\n",
    "gdf = gpd.GeoDataFrame(accidents, geometry=geometry)\n",
    "\n",
    "# Extract and process city names\n",
    "cities = list(map(lambda x: x + \", Poland\", gdf[\"mie_nazwa\"].unique()))\n",
    "print(\"Cities:\")\n",
    "print(cities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65f1124060ba1239",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T05:57:59.983530Z",
     "start_time": "2024-04-29T05:54:44.939172Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_gdfs(city_name: str, h3_resolution: int, year: int, method: str):\n",
    "    osmnx_nodes, osmnx_edges = create_osmnx_dataframes(\n",
    "        df_accidents=accidents, nominatim_city_name=city_name\n",
    "    )\n",
    "    assert (\n",
    "        method in available_h3_embedding_methods\n",
    "    ), f\"H3 embedding method {method} not available, available methods: {available_h3_embedding_methods}\"\n",
    "    hexes: gpd.GeoDataFrame = gpd.read_parquet(\n",
    "        os.path.join(\n",
    "            ORGANIZED_DATASETS_LOCATION,\n",
    "            f\"{convert_nominatim_name_to_filename(resolve_nominatim_city_name(city_name))}/{year}/h{h3_resolution}/{method}/dataset.parquet\",\n",
    "        )\n",
    "    )\n",
    "    hexes = (\n",
    "        hexes.rename(columns={\"region_id\": \"h3_id\"})\n",
    "        .rename_axis(\"region_id\", axis=0)\n",
    "        .drop(columns=\"accidents_count\")\n",
    "    )  # we will be using different aggregation type than the one in the dataset\n",
    "\n",
    "    return dict(osmnx_nodes=osmnx_nodes, osmnx_edges=osmnx_edges, hexes=hexes)\n",
    "\n",
    "\n",
    "# with warnings.catch_warnings():\n",
    "#     warnings.simplefilter(\"ignore\")\n",
    "#     print(\"Creating gdfs...\")\n",
    "#     gdfs_dict = {}\n",
    "#     for city_name in tqdm(cities):\n",
    "#         print(city_name)\n",
    "#         gdfs_dict[city_name] = create_gdfs(\n",
    "#             city_name, H3_RESOLUTION, H3_YEAR, method=H3_EMBEDDING_METHOD\n",
    "#         )\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8c0c660ce7d989",
   "metadata": {},
   "source": [
    "# Usuwanie kolumn, które nie są wspólne dla wszystkich miast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a35b7efb4bb44b0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T05:58:00.000696Z",
     "start_time": "2024-04-29T05:57:59.984256Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_presence_df(gdfs_dict, tested_df_name):\n",
    "    presence_df = pd.DataFrame(\n",
    "        list(\n",
    "            map(\n",
    "                lambda v: (v[0], v[1][tested_df_name].columns.to_list()),\n",
    "                list(gdfs_dict.items()),\n",
    "            )\n",
    "        ),\n",
    "        columns=[\"city_name\", \"col\"],\n",
    "    ).explode(\"col\")\n",
    "    presence_df = (\n",
    "        pd.get_dummies(presence_df, columns=[\"col\"], prefix=\"\", prefix_sep=\"\")\n",
    "        .groupby(\"city_name\")\n",
    "        .sum()\n",
    "    )\n",
    "    return presence_df\n",
    "\n",
    "\n",
    "def filter_presence_df(df):\n",
    "    return df.loc[:, df.sum(axis=0) == len(cities)]\n",
    "\n",
    "\n",
    "def get_common_columns(gdfs_dict, tested_df_name):\n",
    "    df_columns_presence = get_presence_df(gdfs_dict, tested_df_name)\n",
    "    df_common_columns = filter_presence_df(df_columns_presence)\n",
    "    return df_common_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4a9f223fa5dcee89",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T05:58:00.056219Z",
     "start_time": "2024-04-29T05:58:00.001288Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accidents_count</th>\n",
       "      <th>crossing</th>\n",
       "      <th>geometry</th>\n",
       "      <th>give_way</th>\n",
       "      <th>mini_roundabout</th>\n",
       "      <th>motorway_junction</th>\n",
       "      <th>osmid</th>\n",
       "      <th>passing_place</th>\n",
       "      <th>stop</th>\n",
       "      <th>street_count</th>\n",
       "      <th>traffic_signals</th>\n",
       "      <th>turning_circle</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>city_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Kraków, Poland</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Poznań, Poland</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Szczecin, Poland</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Warszawa, Poland</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wrocław, Poland</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  accidents_count  crossing  geometry  give_way  \\\n",
       "city_name                                                         \n",
       "Kraków, Poland                  1         1         1         1   \n",
       "Poznań, Poland                  1         1         1         1   \n",
       "Szczecin, Poland                1         1         1         1   \n",
       "Warszawa, Poland                1         1         1         1   \n",
       "Wrocław, Poland                 1         1         1         1   \n",
       "\n",
       "                  mini_roundabout  motorway_junction  osmid  passing_place  \\\n",
       "city_name                                                                    \n",
       "Kraków, Poland                  1                  1      1              1   \n",
       "Poznań, Poland                  1                  1      1              1   \n",
       "Szczecin, Poland                1                  1      1              1   \n",
       "Warszawa, Poland                1                  1      1              1   \n",
       "Wrocław, Poland                 1                  1      1              1   \n",
       "\n",
       "                  stop  street_count  traffic_signals  turning_circle  x  y  \n",
       "city_name                                                                    \n",
       "Kraków, Poland       1             1                1               1  1  1  \n",
       "Poznań, Poland       1             1                1               1  1  1  \n",
       "Szczecin, Poland     1             1                1               1  1  1  \n",
       "Warszawa, Poland     1             1                1               1  1  1  \n",
       "Wrocław, Poland      1             1                1               1  1  1  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_osmnx_node_common_columns = get_common_columns(gdfs_dict, \"osmnx_nodes\")\n",
    "df_osmnx_node_common_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a7edf632516ddea9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T05:58:00.084518Z",
     "start_time": "2024-04-29T05:58:00.057107Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>access_0</th>\n",
       "      <th>access_destination</th>\n",
       "      <th>access_no</th>\n",
       "      <th>access_permissive</th>\n",
       "      <th>access_yes</th>\n",
       "      <th>bridge_0</th>\n",
       "      <th>bridge_viaduct</th>\n",
       "      <th>bridge_yes</th>\n",
       "      <th>geometry</th>\n",
       "      <th>highway_living_street</th>\n",
       "      <th>...</th>\n",
       "      <th>length</th>\n",
       "      <th>maxspeed</th>\n",
       "      <th>oneway</th>\n",
       "      <th>reversed</th>\n",
       "      <th>tunnel_0</th>\n",
       "      <th>tunnel_building_passage</th>\n",
       "      <th>tunnel_yes</th>\n",
       "      <th>u</th>\n",
       "      <th>v</th>\n",
       "      <th>width</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>city_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Kraków, Poland</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Poznań, Poland</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Szczecin, Poland</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Warszawa, Poland</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wrocław, Poland</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  access_0  access_destination  access_no  access_permissive  \\\n",
       "city_name                                                                      \n",
       "Kraków, Poland           1                   1          1                  1   \n",
       "Poznań, Poland           1                   1          1                  1   \n",
       "Szczecin, Poland         1                   1          1                  1   \n",
       "Warszawa, Poland         1                   1          1                  1   \n",
       "Wrocław, Poland          1                   1          1                  1   \n",
       "\n",
       "                  access_yes  bridge_0  bridge_viaduct  bridge_yes  geometry  \\\n",
       "city_name                                                                      \n",
       "Kraków, Poland             1         1               1           1         1   \n",
       "Poznań, Poland             1         1               1           1         1   \n",
       "Szczecin, Poland           1         1               1           1         1   \n",
       "Warszawa, Poland           1         1               1           1         1   \n",
       "Wrocław, Poland            1         1               1           1         1   \n",
       "\n",
       "                  highway_living_street  ...  length  maxspeed  oneway  \\\n",
       "city_name                                ...                             \n",
       "Kraków, Poland                        1  ...       1         1       1   \n",
       "Poznań, Poland                        1  ...       1         1       1   \n",
       "Szczecin, Poland                      1  ...       1         1       1   \n",
       "Warszawa, Poland                      1  ...       1         1       1   \n",
       "Wrocław, Poland                       1  ...       1         1       1   \n",
       "\n",
       "                  reversed  tunnel_0  tunnel_building_passage  tunnel_yes  u  \\\n",
       "city_name                                                                      \n",
       "Kraków, Poland           1         1                        1           1  1   \n",
       "Poznań, Poland           1         1                        1           1  1   \n",
       "Szczecin, Poland         1         1                        1           1  1   \n",
       "Warszawa, Poland         1         1                        1           1  1   \n",
       "Wrocław, Poland          1         1                        1           1  1   \n",
       "\n",
       "                  v  width  \n",
       "city_name                   \n",
       "Kraków, Poland    1      1  \n",
       "Poznań, Poland    1      1  \n",
       "Szczecin, Poland  1      1  \n",
       "Warszawa, Poland  1      1  \n",
       "Wrocław, Poland   1      1  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_osmnx_edge_common_columns = get_common_columns(gdfs_dict, \"osmnx_edges\")\n",
    "df_osmnx_edge_common_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9388daf91c464dd2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T05:58:00.142308Z",
     "start_time": "2024-04-29T05:58:00.086682Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aeroway_aerodrome</th>\n",
       "      <th>aeroway_helipad</th>\n",
       "      <th>aeroway_runway</th>\n",
       "      <th>amenity_animal_shelter</th>\n",
       "      <th>amenity_arts_centre</th>\n",
       "      <th>amenity_atm</th>\n",
       "      <th>amenity_bank</th>\n",
       "      <th>amenity_bar</th>\n",
       "      <th>amenity_bbq</th>\n",
       "      <th>amenity_bench</th>\n",
       "      <th>...</th>\n",
       "      <th>water_pond</th>\n",
       "      <th>water_reservoir</th>\n",
       "      <th>water_river</th>\n",
       "      <th>water_wastewater</th>\n",
       "      <th>waterway_canal</th>\n",
       "      <th>waterway_ditch</th>\n",
       "      <th>waterway_drain</th>\n",
       "      <th>waterway_river</th>\n",
       "      <th>waterway_stream</th>\n",
       "      <th>waterway_weir</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>city_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Kraków, Poland</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Poznań, Poland</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Szczecin, Poland</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Warszawa, Poland</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wrocław, Poland</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 451 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  aeroway_aerodrome  aeroway_helipad  aeroway_runway  \\\n",
       "city_name                                                              \n",
       "Kraków, Poland                    1                1               1   \n",
       "Poznań, Poland                    1                1               1   \n",
       "Szczecin, Poland                  1                1               1   \n",
       "Warszawa, Poland                  1                1               1   \n",
       "Wrocław, Poland                   1                1               1   \n",
       "\n",
       "                  amenity_animal_shelter  amenity_arts_centre  amenity_atm  \\\n",
       "city_name                                                                    \n",
       "Kraków, Poland                         1                    1            1   \n",
       "Poznań, Poland                         1                    1            1   \n",
       "Szczecin, Poland                       1                    1            1   \n",
       "Warszawa, Poland                       1                    1            1   \n",
       "Wrocław, Poland                        1                    1            1   \n",
       "\n",
       "                  amenity_bank  amenity_bar  amenity_bbq  amenity_bench  ...  \\\n",
       "city_name                                                                ...   \n",
       "Kraków, Poland               1            1            1              1  ...   \n",
       "Poznań, Poland               1            1            1              1  ...   \n",
       "Szczecin, Poland             1            1            1              1  ...   \n",
       "Warszawa, Poland             1            1            1              1  ...   \n",
       "Wrocław, Poland              1            1            1              1  ...   \n",
       "\n",
       "                  water_pond  water_reservoir  water_river  water_wastewater  \\\n",
       "city_name                                                                      \n",
       "Kraków, Poland             1                1            1                 1   \n",
       "Poznań, Poland             1                1            1                 1   \n",
       "Szczecin, Poland           1                1            1                 1   \n",
       "Warszawa, Poland           1                1            1                 1   \n",
       "Wrocław, Poland            1                1            1                 1   \n",
       "\n",
       "                  waterway_canal  waterway_ditch  waterway_drain  \\\n",
       "city_name                                                          \n",
       "Kraków, Poland                 1               1               1   \n",
       "Poznań, Poland                 1               1               1   \n",
       "Szczecin, Poland               1               1               1   \n",
       "Warszawa, Poland               1               1               1   \n",
       "Wrocław, Poland                1               1               1   \n",
       "\n",
       "                  waterway_river  waterway_stream  waterway_weir  \n",
       "city_name                                                         \n",
       "Kraków, Poland                 1                1              1  \n",
       "Poznań, Poland                 1                1              1  \n",
       "Szczecin, Poland               1                1              1  \n",
       "Warszawa, Poland               1                1              1  \n",
       "Wrocław, Poland                1                1              1  \n",
       "\n",
       "[5 rows x 451 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hexes_common_columns = get_common_columns(gdfs_dict, \"hexes\")\n",
    "df_hexes_common_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "2fdc8136326251d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T05:58:00.358809Z",
     "start_time": "2024-04-29T05:58:00.142909Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting columns that are not common for all cities...\n"
     ]
    }
   ],
   "source": [
    "print(\"Deleting columns that are not common for all cities...\")\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    for gdf_for_city in gdfs_dict.values():\n",
    "        osmnx_nodes = gdf_for_city[\"osmnx_nodes\"]\n",
    "        osmnx_edges = gdf_for_city[\"osmnx_edges\"]\n",
    "        hexes = gdf_for_city[\"hexes\"]\n",
    "\n",
    "        osmnx_nodes.drop(\n",
    "            columns=osmnx_nodes.columns.difference(\n",
    "                df_osmnx_node_common_columns.columns\n",
    "            ),\n",
    "            inplace=True,\n",
    "        )\n",
    "        osmnx_edges.drop(\n",
    "            columns=osmnx_edges.columns.difference(\n",
    "                df_osmnx_edge_common_columns.columns\n",
    "            ),\n",
    "            inplace=True,\n",
    "        )\n",
    "        hexes.drop(\n",
    "            columns=hexes.columns.difference(df_hexes_common_columns.columns),\n",
    "            inplace=True,\n",
    "        )\n",
    "\n",
    "        gdf_for_city[\"osmnx_nodes\"] = osmnx_nodes.reindex(\n",
    "            columns=df_osmnx_node_common_columns.columns\n",
    "        )\n",
    "        gdf_for_city[\"osmnx_edges\"] = osmnx_edges.reindex(\n",
    "            columns=df_osmnx_edge_common_columns.columns\n",
    "        )\n",
    "        gdf_for_city[\"hexes\"] = hexes.reindex(columns=df_hexes_common_columns.columns)\n",
    "\n",
    "        gdf_for_city[\"controller\"] = GraphLayerController(\n",
    "            gdf_for_city[\"hexes\"],\n",
    "            gdf_for_city[\"osmnx_nodes\"],\n",
    "            gdf_for_city[\"osmnx_edges\"],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e0aac7a866a12b15",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T05:58:00.376920Z",
     "start_time": "2024-04-29T05:58:00.359616Z"
    }
   },
   "outputs": [],
   "source": [
    "def patch_hexes_with_y(\n",
    "    osmnx_nodes: gpd.GeoDataFrame,\n",
    "    hexes: gpd.GeoDataFrame,\n",
    "    controller: GraphLayerController,\n",
    "):\n",
    "    virtual_edges = controller.get_virtual_edges_to_hexes(SourceType.OSMNX_NODES)\n",
    "    hexes_with_y = cast(\n",
    "        gpd.GeoDataFrame,\n",
    "        hexes.merge(\n",
    "            virtual_edges.merge(osmnx_nodes, left_on=\"source_id\", right_index=True)[\n",
    "                [\"region_id\", \"accidents_count\"]\n",
    "            ]\n",
    "            .groupby(\"region_id\")\n",
    "            .sum(),\n",
    "            left_index=True,\n",
    "            right_index=True,\n",
    "            how=\"left\",\n",
    "        ).fillna(0),\n",
    "    )\n",
    "    hexes_with_y[\"accident_occured\"] = (hexes_with_y[\"accidents_count\"] > 0).astype(int)\n",
    "    hexes_with_y.drop(columns=\"accidents_count\", inplace=True)\n",
    "    controller.hexes_gdf = hexes_with_y\n",
    "    controller._hexes_centroids_gdf = controller._create_hexes_centroids_gdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7348370ea8cdaece",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T05:58:00.689761Z",
     "start_time": "2024-04-29T05:58:00.377730Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patching hexes with y...\n"
     ]
    }
   ],
   "source": [
    "print(\"Patching hexes with y...\")\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    for gdfs in gdfs_dict.values():\n",
    "        patch_hexes_with_y(gdfs[\"osmnx_nodes\"], gdfs[\"hexes\"], gdfs[\"controller\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "aa099bf6c66b97cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T05:58:01.878441Z",
     "start_time": "2024-04-29T05:58:00.690710Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_torch_geometric_hetero_data(\n",
    "    osmnx_nodes, osmnx_edges, hexes, controller: GraphLayerController\n",
    "):\n",
    "    edges_attr_columns = osmnx_edges.columns[\n",
    "        ~osmnx_edges.columns.isin([\"u\", \"v\", \"key\", \"geometry\"])\n",
    "    ]\n",
    "    nodes_attr_columns = osmnx_nodes.columns[\n",
    "        ~osmnx_nodes.columns.isin([\"geometry\", \"x\", \"y\", \"osmid\", \"accidents_count\"])\n",
    "    ]\n",
    "    hexes_attr_columns = hexes.columns[~hexes.columns.isin([\"geometry\", \"h3_id\"])]\n",
    "\n",
    "    data = create_hetero_data(\n",
    "        controller,\n",
    "        hexes_attrs_columns_names=hexes_attr_columns,\n",
    "        osmnx_edge_attrs_columns_names=edges_attr_columns,\n",
    "        osmnx_node_attrs_columns_names=nodes_attr_columns,\n",
    "        virtual_edge_attrs_columns_names=[],\n",
    "        hexes_y_columns_names=[\"accident_occured\"],\n",
    "    )\n",
    "    return data\n",
    "\n",
    "\n",
    "data_dict = {\n",
    "    city_name: create_torch_geometric_hetero_data(**gdfs)\n",
    "    for city_name, gdfs in gdfs_dict.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735adf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_elements_right(lst):\n",
    "    shifted_lst = [lst[-1]] + lst[:-1]\n",
    "    return shifted_lst\n",
    "\n",
    "\n",
    "cities_names_list = list(data_dict.keys())\n",
    "\n",
    "# val + test\n",
    "folds_tuples = list(zip(shift_elements_right(cities_names_list), cities_names_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "54cb3c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folds tuples (val, test)\n",
      "[('Warszawa, Poland', 'Wrocław, Poland'), ('Wrocław, Poland', 'Szczecin, Poland'), ('Szczecin, Poland', 'Poznań, Poland'), ('Poznań, Poland', 'Kraków, Poland'), ('Kraków, Poland', 'Warszawa, Poland')]\n"
     ]
    }
   ],
   "source": [
    "print(\"Folds tuples (val, test)\")\n",
    "print(folds_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9435fc49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hidden_channels': 10, 'learning_rate': 1e-05, 'num_conv_layers': 1, 'lin_layer_size': 8, 'num_lin_layers': 0, 'weight_decay': 1e-05}\n",
      "{'hidden_channels': 30, 'learning_rate': 1e-05, 'num_conv_layers': 3, 'lin_layer_size': 32, 'num_lin_layers': 2, 'weight_decay': 1e-05}\n",
      "{'hidden_channels': 50, 'learning_rate': 1e-05, 'num_conv_layers': 5, 'lin_layer_size': 128, 'num_lin_layers': 4, 'weight_decay': 1e-05}\n"
     ]
    }
   ],
   "source": [
    "configs_list = [\n",
    "    {\n",
    "        \"hidden_channels\": 10,\n",
    "        \"learning_rate\": 1e-5,\n",
    "        \"num_conv_layers\": 1,\n",
    "        \"lin_layer_size\": 8,\n",
    "        \"num_lin_layers\": 0,\n",
    "        \"weight_decay\": 1e-5,\n",
    "    },\n",
    "    {\n",
    "        \"hidden_channels\": 30,\n",
    "        \"learning_rate\": 1e-5,\n",
    "        \"num_conv_layers\": 3,\n",
    "        \"lin_layer_size\": 32,\n",
    "        \"num_lin_layers\": 2,\n",
    "        \"weight_decay\": 1e-5,\n",
    "    },\n",
    "    {\n",
    "        \"hidden_channels\": 50,\n",
    "        \"learning_rate\": 1e-5,\n",
    "        \"num_conv_layers\": 5,\n",
    "        \"lin_layer_size\": 128,\n",
    "        \"num_lin_layers\": 4,\n",
    "        \"weight_decay\": 1e-5,\n",
    "    }\n",
    "]\n",
    "\n",
    "for config in configs_list:\n",
    "    print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "312d98f295cfeb0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T07:59:07.807822Z",
     "start_time": "2024-04-29T07:58:54.359959Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Natalia\\Desktop\\gradient_new\\gradient\\.venv\\lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\Natalia\\Desktop\\gradient_new\\gradient\\.venv\\lib\\site-packages\\pytorch_lightning\\utilities\\model_summary\\model_summary.py:410: UserWarning: A layer with UninitializedParameter was found. Thus, the total number of parameters detected may be inaccurate.\n",
      "  warning_cache.warn(\n",
      "\n",
      "  | Name  | Type      | Params\n",
      "------------------------------------\n",
      "0 | model | HeteroGNN | 14.2 K\n",
      "------------------------------------\n",
      "14.2 K    Trainable params\n",
      "0         Non-trainable params\n",
      "14.2 K    Total params\n",
      "0.057     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da6d66bbac5548baaaea90172640986b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Natalia\\Desktop\\gradient_new\\gradient\\.venv\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\Natalia\\Desktop\\gradient_new\\gradient\\.venv\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0be51f86725459d9d7248849151ddfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=300` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model path:\n",
      "res2024_05_22_09_17_18/checkpoints\\model-checkpoint-epoch-epoch=296.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Natalia\\Desktop\\gradient_new\\gradient\\.venv\\lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type      | Params\n",
      "------------------------------------\n",
      "0 | model | HeteroGNN | 14.2 K\n",
      "------------------------------------\n",
      "14.2 K    Trainable params\n",
      "0         Non-trainable params\n",
      "14.2 K    Total params\n",
      "0.057     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "213dc4ad4bfe40cc8642e8a464bc0a02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Natalia\\Desktop\\gradient_new\\gradient\\.venv\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\Natalia\\Desktop\\gradient_new\\gradient\\.venv\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d358cca3b25c440da671ccca5a0ae082",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=300` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model path:\n",
      "res2024_05_22_09_31_25/checkpoints\\model-checkpoint-epoch-epoch=299.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Natalia\\Desktop\\gradient_new\\gradient\\.venv\\lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type      | Params\n",
      "------------------------------------\n",
      "0 | model | HeteroGNN | 14.2 K\n",
      "------------------------------------\n",
      "14.2 K    Trainable params\n",
      "0         Non-trainable params\n",
      "14.2 K    Total params\n",
      "0.057     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e979d0bb3ef7414ab1f068fb9b57b68e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Natalia\\Desktop\\gradient_new\\gradient\\.venv\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\Natalia\\Desktop\\gradient_new\\gradient\\.venv\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebf973b774db4b09b45f5de0ea570cb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=300` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model path:\n",
      "res2024_05_22_09_40_36/checkpoints\\model-checkpoint-epoch-epoch=299.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Natalia\\Desktop\\gradient_new\\gradient\\.venv\\lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type      | Params\n",
      "------------------------------------\n",
      "0 | model | HeteroGNN | 14.2 K\n",
      "------------------------------------\n",
      "14.2 K    Trainable params\n",
      "0         Non-trainable params\n",
      "14.2 K    Total params\n",
      "0.057     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "460256d9c5624347992b2111dae59fc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Natalia\\Desktop\\gradient_new\\gradient\\.venv\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\Natalia\\Desktop\\gradient_new\\gradient\\.venv\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7db9d79d939e42f489e81bda53ba4719",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=300` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model path:\n",
      "res2024_05_22_09_50_13/checkpoints\\model-checkpoint-epoch-epoch=298.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Natalia\\Desktop\\gradient_new\\gradient\\.venv\\lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type      | Params\n",
      "------------------------------------\n",
      "0 | model | HeteroGNN | 14.2 K\n",
      "------------------------------------\n",
      "14.2 K    Trainable params\n",
      "0         Non-trainable params\n",
      "14.2 K    Total params\n",
      "0.057     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "463eb1606c834518bbc6ab4bad80f5c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Natalia\\Desktop\\gradient_new\\gradient\\.venv\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\Natalia\\Desktop\\gradient_new\\gradient\\.venv\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad8ea34b145d4fb28ffe03663230ab86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=300` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model path:\n",
      "res2024_05_22_10_00_54/checkpoints\\model-checkpoint-epoch-epoch=298.ckpt\n",
      "Config: {'hidden_channels': 10, 'learning_rate': 1e-05, 'num_conv_layers': 1, 'lin_layer_size': 8, 'num_lin_layers': 0, 'weight_decay': 1e-05}\n",
      "Mean AUC: 0.7579575483505506\n",
      "Mean Accuracy: 0.6100248464264086\n",
      "Mean F1: 0.48377396916813814\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Natalia\\Desktop\\gradient_new\\gradient\\.venv\\lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type      | Params\n",
      "------------------------------------\n",
      "0 | model | HeteroGNN | 60.0 K\n",
      "------------------------------------\n",
      "60.0 K    Trainable params\n",
      "0         Non-trainable params\n",
      "60.0 K    Total params\n",
      "0.240     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8de12617dc04da58bd1aef8755216e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Natalia\\Desktop\\gradient_new\\gradient\\.venv\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\Natalia\\Desktop\\gradient_new\\gradient\\.venv\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc8f35317e9d4807aac16d35bbe34835",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=300` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model path:\n",
      "res2024_05_22_10_10_03/checkpoints\\model-checkpoint-epoch-epoch=278.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Natalia\\Desktop\\gradient_new\\gradient\\.venv\\lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type      | Params\n",
      "------------------------------------\n",
      "0 | model | HeteroGNN | 60.0 K\n",
      "------------------------------------\n",
      "60.0 K    Trainable params\n",
      "0         Non-trainable params\n",
      "60.0 K    Total params\n",
      "0.240     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d97599a59a8248298ab2f1d577a28e88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Natalia\\Desktop\\gradient_new\\gradient\\.venv\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\Natalia\\Desktop\\gradient_new\\gradient\\.venv\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a0ed327796d4cfe901bbbf705f3414c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=300` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model path:\n",
      "res2024_05_22_10_44_01/checkpoints\\model-checkpoint-epoch-epoch=299.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Natalia\\Desktop\\gradient_new\\gradient\\.venv\\lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type      | Params\n",
      "------------------------------------\n",
      "0 | model | HeteroGNN | 60.0 K\n",
      "------------------------------------\n",
      "60.0 K    Trainable params\n",
      "0         Non-trainable params\n",
      "60.0 K    Total params\n",
      "0.240     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a83e9311e06441fa63fabe0e49903fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Natalia\\Desktop\\gradient_new\\gradient\\.venv\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\Natalia\\Desktop\\gradient_new\\gradient\\.venv\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c69830b43af945388647e5e537507761",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=300` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model path:\n",
      "res2024_05_22_11_08_59/checkpoints\\model-checkpoint-epoch-epoch=299.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Natalia\\Desktop\\gradient_new\\gradient\\.venv\\lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type      | Params\n",
      "------------------------------------\n",
      "0 | model | HeteroGNN | 60.0 K\n",
      "------------------------------------\n",
      "60.0 K    Trainable params\n",
      "0         Non-trainable params\n",
      "60.0 K    Total params\n",
      "0.240     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e8ea5d7086f4ff4a3f207fa1bda2c0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Natalia\\Desktop\\gradient_new\\gradient\\.venv\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\Natalia\\Desktop\\gradient_new\\gradient\\.venv\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "848838c5fad34a78a119fec5f0bfb5db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=300` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model path:\n",
      "res2024_05_22_11_32_53/checkpoints\\model-checkpoint-epoch-epoch=299.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Natalia\\Desktop\\gradient_new\\gradient\\.venv\\lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type      | Params\n",
      "------------------------------------\n",
      "0 | model | HeteroGNN | 60.0 K\n",
      "------------------------------------\n",
      "60.0 K    Trainable params\n",
      "0         Non-trainable params\n",
      "60.0 K    Total params\n",
      "0.240     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0e851f6fa2544bead4e9973e4d0ad77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Natalia\\Desktop\\gradient_new\\gradient\\.venv\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\Natalia\\Desktop\\gradient_new\\gradient\\.venv\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69d439ef773441b6bddf9fe774dad924",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=300` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model path:\n",
      "res2024_05_22_12_00_17/checkpoints\\model-checkpoint-epoch-epoch=297.ckpt\n",
      "Config: {'hidden_channels': 30, 'learning_rate': 1e-05, 'num_conv_layers': 3, 'lin_layer_size': 32, 'num_lin_layers': 2, 'weight_decay': 1e-05}\n",
      "Mean AUC: 0.813785759984907\n",
      "Mean Accuracy: 0.7482739187074932\n",
      "Mean F1: 0.6170900183333908\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Natalia\\Desktop\\gradient_new\\gradient\\.venv\\lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type      | Params\n",
      "------------------------------------\n",
      "0 | model | HeteroGNN | 251 K \n",
      "------------------------------------\n",
      "251 K     Trainable params\n",
      "0         Non-trainable params\n",
      "251 K     Total params\n",
      "1.006     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "921a3536738947fa9bef2da16e0a21ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Natalia\\Desktop\\gradient_new\\gradient\\.venv\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\Natalia\\Desktop\\gradient_new\\gradient\\.venv\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "948740b64d9c44fbaad569eb9afd58f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=300` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model path:\n",
      "res2024_05_22_12_21_24/checkpoints\\model-checkpoint-epoch-epoch=32.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Natalia\\Desktop\\gradient_new\\gradient\\.venv\\lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type      | Params\n",
      "------------------------------------\n",
      "0 | model | HeteroGNN | 251 K \n",
      "------------------------------------\n",
      "251 K     Trainable params\n",
      "0         Non-trainable params\n",
      "251 K     Total params\n",
      "1.006     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0742a79c7e5c4928b70351a0eb73f915",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Natalia\\Desktop\\gradient_new\\gradient\\.venv\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\Natalia\\Desktop\\gradient_new\\gradient\\.venv\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a2953780246463f9114c9f2aea44151",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=300` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model path:\n",
      "res2024_05_22_13_25_30/checkpoints\\model-checkpoint-epoch-epoch=195.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Natalia\\Desktop\\gradient_new\\gradient\\.venv\\lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type      | Params\n",
      "------------------------------------\n",
      "0 | model | HeteroGNN | 251 K \n",
      "------------------------------------\n",
      "251 K     Trainable params\n",
      "0         Non-trainable params\n",
      "251 K     Total params\n",
      "1.006     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c89e22e703a4cbe990dd570534e5285",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Natalia\\Desktop\\gradient_new\\gradient\\.venv\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\Natalia\\Desktop\\gradient_new\\gradient\\.venv\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "400a46cb8bb64f0b8384465843553cda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=300` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model path:\n",
      "res2024_05_22_14_38_18/checkpoints\\model-checkpoint-epoch-epoch=297.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Natalia\\Desktop\\gradient_new\\gradient\\.venv\\lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type      | Params\n",
      "------------------------------------\n",
      "0 | model | HeteroGNN | 251 K \n",
      "------------------------------------\n",
      "251 K     Trainable params\n",
      "0         Non-trainable params\n",
      "251 K     Total params\n",
      "1.006     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c57191c7d204c7ab0301b2a58b3d3cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Natalia\\Desktop\\gradient_new\\gradient\\.venv\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\Natalia\\Desktop\\gradient_new\\gradient\\.venv\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3baf0fa9f6f49c093c41f3bcbbf0494",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=300` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model path:\n",
      "res2024_05_22_15_54_14/checkpoints\\model-checkpoint-epoch-epoch=273.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Natalia\\Desktop\\gradient_new\\gradient\\.venv\\lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type      | Params\n",
      "------------------------------------\n",
      "0 | model | HeteroGNN | 251 K \n",
      "------------------------------------\n",
      "251 K     Trainable params\n",
      "0         Non-trainable params\n",
      "251 K     Total params\n",
      "1.006     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10398449874a4891880c93cfa5719085",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Natalia\\Desktop\\gradient_new\\gradient\\.venv\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\Natalia\\Desktop\\gradient_new\\gradient\\.venv\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f8741c89d6e40fca3439a88a749bf51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=300` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model path:\n",
      "res2024_05_22_20_04_03/checkpoints\\model-checkpoint-epoch-epoch=187.ckpt\n",
      "Config: {'hidden_channels': 50, 'learning_rate': 1e-05, 'num_conv_layers': 5, 'lin_layer_size': 128, 'num_lin_layers': 4, 'weight_decay': 1e-05}\n",
      "Mean AUC: 0.8422804209997838\n",
      "Mean Accuracy: 0.7746590601440132\n",
      "Mean F1: 0.6227139914656291\n",
      "--------------------------------------------------\n",
      "Overall Results\n",
      "Mean AUC: 0.8046745764450804 ± 0.03502234488700992\n",
      "Mean Accuracy: 0.7109859417593051 ± 0.07219834254916535\n",
      "Mean F1: 0.5745259929890527 ± 0.06421243202133534\n"
     ]
    }
   ],
   "source": [
    "# from wandb.util import generate_id\n",
    "\n",
    "from src.training.train import train\n",
    "\n",
    "\n",
    "def run_k_fold(hparams):\n",
    "    # run = wandb.init()\n",
    "    epochs = EPOCHS\n",
    "\n",
    "    # config = wandb.config\n",
    "\n",
    "    lin_layer_sizes = [hparams['lin_layer_size']] * hparams['num_lin_layers']\n",
    "\n",
    "\n",
    "    hparams = {\n",
    "        \"hidden_channels\": hparams['hidden_channels'],\n",
    "        \"lr\": hparams['learning_rate'],\n",
    "        \"num_conv_layers\": hparams['num_conv_layers'],\n",
    "        \"lin_layer_sizes\": lin_layer_sizes,\n",
    "        \"weight_decay\": hparams['weight_decay'],\n",
    "    }\n",
    "\n",
    "    aucs = []\n",
    "    accuracies = []\n",
    "    f1s = []\n",
    "\n",
    "    # fold_group_id = generate_id()\n",
    "    TRAIN_SAVE_DIR='res'\n",
    "    for index, (val_city_name, test_city_name) in enumerate(folds_tuples):\n",
    "        val_data = [data_dict[val_city_name].to(\"cpu\").clone()]\n",
    "        train_data = [\n",
    "            v.to(\"cpu\").clone()\n",
    "            for k, v in data_dict.items()\n",
    "            if k != val_city_name and k != test_city_name\n",
    "        ]\n",
    "        test_data = data_dict[test_city_name].to(\"cpu\").clone()\n",
    "\n",
    "        auc, accuracy, f1, model_path = train(\n",
    "            train_data=train_data,\n",
    "            val_data=val_data,\n",
    "            test_data=test_data,\n",
    "            epochs=epochs,\n",
    "            hparams=hparams,\n",
    "            train_save_dir=TRAIN_SAVE_DIR,\n",
    "        )\n",
    "        # run.log_model(\n",
    "        #     path=model_path,\n",
    "        #     name=f\"model_{fold_group_id}_fold_{index}\",\n",
    "        # )\n",
    "        # run.log({f\"auc_fold_{index}\": auc})\n",
    "        # run.log({f\"accuracy_fold_{index}\": accuracy})\n",
    "        # run.log({f\"f1_fold_{index}\": f1})\n",
    "\n",
    "        aucs.append(auc)\n",
    "        accuracies.append(accuracy)\n",
    "        f1s.append(f1)\n",
    "\n",
    "    mean_auc = sum(aucs) / len(aucs)\n",
    "    mean_accuracy = sum(accuracies) / len(accuracies)\n",
    "    mean_f1 = sum(f1s) / len(f1s)\n",
    "    # run.log({\"mean_auc\": mean_auc})\n",
    "    # run.log({\"mean_accuracy\": mean_accuracy})\n",
    "    # run.log({\"mean_f1\": mean_f1})\n",
    "    return {\n",
    "        \"mean_auc\": mean_auc,\n",
    "        \"mean_accuracy\": mean_accuracy,\n",
    "        \"mean_f1\": mean_f1,\n",
    "        \"aucs\": aucs,\n",
    "        \"accuracies\": accuracies,\n",
    "        \"f1s\": f1s,\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    results = []\n",
    "    # print(sweep_configs)\n",
    "    for config in configs_list:\n",
    "        result = run_k_fold(config)\n",
    "        results.append(result)\n",
    "        print(f\"Config: {config}\")\n",
    "        print(f\"Mean AUC: {result['mean_auc']}\")\n",
    "        print(f\"Mean Accuracy: {result['mean_accuracy']}\")\n",
    "        print(f\"Mean F1: {result['mean_f1']}\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    # Optionally, you can display aggregated results in a more comprehensive format\n",
    "    mean_aucs = [res[\"mean_auc\"] for res in results]\n",
    "    mean_accuracies = [res[\"mean_accuracy\"] for res in results]\n",
    "    mean_f1s = [res[\"mean_f1\"] for res in results]\n",
    "\n",
    "    print(\"Overall Results\")\n",
    "    print(f\"Mean AUC: {np.mean(mean_aucs)} ± {np.std(mean_aucs)}\")\n",
    "    print(f\"Mean Accuracy: {np.mean(mean_accuracies)} ± {np.std(mean_accuracies)}\")\n",
    "    print(f\"Mean F1: {np.mean(mean_f1s)} ± {np.std(mean_f1s)}\")\n",
    "    return result\n",
    "        \n",
    "\n",
    "\n",
    "result = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d74e1bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json \n",
    "\n",
    "# with open('base_results.json', 'w') as file:\n",
    "#     json.dump(result, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c797bd0",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "### Dodanie ortofotomap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4187bb69",
   "metadata": {},
   "source": [
    "dla jednego hexsa embeding jest 197x768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2bded49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test = pd.read_parquet('C:/Users/Natalia/Desktop/gradient_new/gradient/data/maps_embeddings/regions_8_emb_Poznań.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "99cd02a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(151296,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.iloc()[0]['emb'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1848c5c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gdfs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrocław, Poland\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [03:08<12:35, 188.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Szczecin, Poland\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [05:41<08:22, 167.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poznań, Poland\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [12:07<08:54, 267.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kraków, Poland\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [15:40<04:05, 245.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warszawa, Poland\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [27:37<00:00, 331.56s/it]\n"
     ]
    }
   ],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    print(\"Creating gdfs...\")\n",
    "    gdfs_dict2 = {}\n",
    "    for city_name in tqdm(cities):\n",
    "        print(city_name)\n",
    "        gdfs_dict2[city_name] = create_gdfs(\n",
    "            city_name, H3_RESOLUTION, H3_YEAR, method=H3_EMBEDDING_METHOD\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f73c120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrocław, Poland\n",
      "Szczecin, Poland\n",
      "Poznań, Poland\n",
      "Kraków, Poland\n",
      "Warszawa, Poland\n"
     ]
    }
   ],
   "source": [
    "import copy \n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "\n",
    "def get_presence_df(gdfs_dict, tested_df_name):\n",
    "    \n",
    "    presence_df = pd.DataFrame(\n",
    "        list(\n",
    "            map(\n",
    "                lambda v: (v[0], v[1][tested_df_name].columns.to_list()),\n",
    "                list(gdfs_dict.items()),\n",
    "            )\n",
    "        ),\n",
    "        columns=[\"city_name\", \"col\"],\n",
    "    ).explode(\"col\")\n",
    "    presence_df = (\n",
    "        pd.get_dummies(presence_df, columns=[\"col\"], prefix=\"\", prefix_sep=\"\")\n",
    "        .groupby(\"city_name\")\n",
    "        .sum()\n",
    "    )\n",
    "    return presence_df\n",
    "\n",
    "\n",
    "def filter_presence_df(df):\n",
    "    return df.loc[:, df.sum(axis=0) == len(cities)]\n",
    "\n",
    "\n",
    "def get_common_columns(gdfs_dict, tested_df_name):\n",
    "    df_columns_presence = get_presence_df(gdfs_dict, tested_df_name)\n",
    "    df_common_columns = filter_presence_df(df_columns_presence)\n",
    "    return df_common_columns\n",
    "\n",
    "df_osmnx_node_common_columns = get_common_columns(gdfs_dict2, \"osmnx_nodes\")\n",
    "# df_osmnx_node_common_columns\n",
    "\n",
    "df_osmnx_edge_common_columns = get_common_columns(gdfs_dict2, \"osmnx_edges\")\n",
    "# df_osmnx_edge_common_columns\n",
    "\n",
    "\n",
    "df_hexes_common_columns = get_common_columns(gdfs_dict2, \"hexes\")\n",
    "# df_hexes_common_columns\n",
    "# print(\"Deleting columns that are not common for all cities...\")\n",
    "\n",
    "\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    for gdf_for_city_key in list(gdfs_dict2.keys()):\n",
    "        print(gdf_for_city_key)\n",
    "        gdf_for_city = gdfs_dict2[gdf_for_city_key]\n",
    "        city = gdf_for_city_key.split(',')[0]\n",
    "        # print(gdf_for_city)\n",
    "        emb_8 = pd.read_parquet(f'C:/Users/Natalia/Desktop/gradient_new/gradient/data/maps_embeddings/regions_8_emb_{city}.parquet')\n",
    "\n",
    "        osmnx_nodes = gdf_for_city[\"osmnx_nodes\"]\n",
    "        osmnx_edges = gdf_for_city[\"osmnx_edges\"]\n",
    "        hexes = gdf_for_city[\"hexes\"]\n",
    "\n",
    "        osmnx_nodes.drop(\n",
    "            columns=osmnx_nodes.columns.difference(\n",
    "                df_osmnx_node_common_columns.columns\n",
    "            ),\n",
    "            inplace=True,\n",
    "        )\n",
    "        osmnx_edges.drop(\n",
    "            columns=osmnx_edges.columns.difference(\n",
    "                df_osmnx_edge_common_columns.columns\n",
    "            ),\n",
    "            inplace=True,\n",
    "        )\n",
    "        hexes.drop(\n",
    "            columns=hexes.columns.difference(df_hexes_common_columns.columns),\n",
    "            inplace=True,\n",
    "        )\n",
    "        # display('h3_id' in hexes.columns)\n",
    "        #  dodac ortofotomapy\n",
    "        gdf_for_city[\"osmnx_nodes\"] = osmnx_nodes.reindex(\n",
    "            columns=df_osmnx_node_common_columns.columns\n",
    "        )\n",
    "        gdf_for_city[\"osmnx_edges\"] = osmnx_edges.reindex(\n",
    "            columns=df_osmnx_edge_common_columns.columns\n",
    "        )\n",
    "        # gdf_for_city[\"hexes\"] = hexes.reindex(columns=df_hexes_common_columns.columns)\n",
    "\n",
    "        # Path to your GeoJSON file\n",
    "        geojson_file = f'C:/Users/Natalia/Desktop/gradient_new/gradient/data/maps_embeddings/regions_9_emb_{city}.geojson'\n",
    "\n",
    "        # Read the GeoJSON file\n",
    "        gdf_8_to_9 = gpd.read_file(geojson_file)[['region_id', 'region_id_res_8']]\n",
    "        gdf_8_to_9.rename(columns={'region_id': 'h3_id'}, inplace=True) \n",
    "        gdf_8_to_9.rename(columns={'region_id_res_8': 'region_id'}, inplace=True)\n",
    "\n",
    "        emb_9  = pd.merge(gdf_8_to_9, emb_8, on='region_id', how='left')  # You can change 'inner' to 'outer', 'left', or 'right' depending on your needs\n",
    "\n",
    "        emb_9.drop(\n",
    "            columns='region_id',\n",
    "            inplace=True,\n",
    "        )\n",
    "        # Display the GeoDataFrame\n",
    "  \n",
    "        gdf_for_city[\"hexes\"] = pd.merge(hexes, emb_9, on='h3_id', how='left')  # You can change 'inner' to 'outer', 'left', or 'right' depending on your needs\n",
    "\n",
    "        zeros_array = np.zeros(151296)\n",
    "\n",
    "        # Find the indices of rows with NaN values in the specified column\n",
    "        nan_indices = gdf_for_city[\"hexes\"].index[gdf_for_city[\"hexes\"]['emb'].isna()]\n",
    "\n",
    "        # Fill NaN values in the specified column with the zeros array\n",
    "        for idx in nan_indices:\n",
    "            gdf_for_city[\"hexes\"].at[idx, 'emb'] = zeros_array\n",
    "\n",
    "        # expanded_emb = gdf_for_city[\"hexes\"]['emb'].apply(pd.Series)\n",
    "\n",
    "        # # Nadanie nowych nazw kolumn\n",
    "        # expanded_emb.columns = [f'emb_{i+1}' for i in range(len(expanded_emb.columns))]\n",
    "\n",
    "        # # Połączenie DataFrame'ów\n",
    "        # gdf_for_city[\"hexes\"] = pd.concat([gdf_for_city[\"hexes\"].drop(columns=['emb']), expanded_emb], axis=1)\n",
    "\n",
    "\n",
    "        gdf_for_city[\"hexes\"] = gdf_for_city[\"hexes\"].reindex(columns=list(df_hexes_common_columns.columns)+['emb'])\n",
    "\n",
    "        gdf_for_city[\"hexes\"] = gdf_for_city[\"hexes\"].reset_index().set_index('index').rename_axis('region_id')\n",
    "        \n",
    "        gdf_for_city[\"controller\"] = GraphLayerController(\n",
    "            gdf_for_city[\"hexes\"],\n",
    "            gdf_for_city[\"osmnx_nodes\"],\n",
    "            gdf_for_city[\"osmnx_edges\"],\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1cd1a244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patching hexes with y...\n"
     ]
    }
   ],
   "source": [
    "# gdfs_dict22 = copy.deepcopy(gdfs_dict2)\n",
    "\n",
    "\n",
    "def patch_hexes_with_y(\n",
    "    osmnx_nodes: gpd.GeoDataFrame,\n",
    "    hexes: gpd.GeoDataFrame,\n",
    "    controller: GraphLayerController,\n",
    "):\n",
    "    virtual_edges = controller.get_virtual_edges_to_hexes(SourceType.OSMNX_NODES)\n",
    "    hexes_with_y = cast(\n",
    "        gpd.GeoDataFrame,\n",
    "        hexes.merge(\n",
    "            virtual_edges.merge(osmnx_nodes, left_on=\"source_id\", right_index=True)[\n",
    "                [\"region_id\", \"accidents_count\"]\n",
    "            ]\n",
    "            .groupby(\"region_id\")\n",
    "            .sum(),\n",
    "            left_index=True,\n",
    "            right_index=True,\n",
    "            how=\"left\",\n",
    "        ).fillna(0),\n",
    "    )\n",
    "    hexes_with_y[\"accident_occured\"] = (hexes_with_y[\"accidents_count\"] > 0).astype(int)\n",
    "    hexes_with_y.drop(columns=\"accidents_count\", inplace=True)\n",
    "    controller.hexes_gdf = hexes_with_y\n",
    "    controller._hexes_centroids_gdf = controller._create_hexes_centroids_gdf()\n",
    "    \n",
    "print(\"Patching hexes with y...\")\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    for gdfs in gdfs_dict2.values():\n",
    "        patch_hexes_with_y(gdfs[\"osmnx_nodes\"], gdfs[\"hexes\"], gdfs[\"controller\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5e316291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage before deletion: 202.16015625 MB\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "import gc\n",
    "def memory_usage():\n",
    "    process = psutil.Process()\n",
    "    mem_info = process.memory_info()\n",
    "    return mem_info.rss / 1024 ** 2  # Return memory usage in MB\n",
    "\n",
    "print(\"Memory usage before deletion:\", memory_usage(), \"MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "378a7ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after deletion: 2152.06640625 MB\n"
     ]
    }
   ],
   "source": [
    "del zeros_array, nan_indices, gdfs\n",
    "gc.collect()\n",
    "print(\"Memory usage after deletion:\", memory_usage(), \"MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8a76096a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage before deletion: 2322.4296875 MB\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'city' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMemory usage before deletion:\u001b[39m\u001b[38;5;124m\"\u001b[39m, memory_usage(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# del df_osmnx_node_common_columns, df_osmnx_edge_common_columns, df_hexes_common_columns, city, emb_8, osmnx_nodes, osmnx_edges, hexes, geojson_file, gdf_8_to_9, emb_9, gdf_for_city, zeros_array, nan_indices, gdfs\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m city\n\u001b[0;32m     12\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMemory usage after deletion:\u001b[39m\u001b[38;5;124m\"\u001b[39m, memory_usage(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'city' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# del df_osmnx_node_common_columns, df_osmnx_edge_common_columns, df_hexes_common_columns, city, emb_8, osmnx_nodes, osmnx_edges, hexes, geojson_file, gdf_8_to_9, emb_9, gdf_for_city, zeros_array, nan_indices, gdfs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc1afc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.set_printoptions(threshold=np.inf)\n",
    "# dataset = copy.deepcopy(gdfs_dict2)\n",
    "# # # Save dataset to HDF5 file\n",
    "# with h5py.File('dataset_citis.h5', 'w') as f:\n",
    "#     for column, rows in dataset.items():\n",
    "#         for row, geo_df in rows.items():\n",
    "#             # Create a dataset name that includes both column and row names\n",
    "#             dataset_name = f'{column}/{row}'\n",
    "#             # Convert the GeoDataFrame to a format suitable for saving\n",
    "#             # For example, you can convert it to a CSV string\n",
    "#             csv_string = geo_df.to_csv()\n",
    "            \n",
    "#             if row=='hexes':\n",
    "#                 print('h')\n",
    "#                 # Convert the 'emb' column from string representations to NumPy arrays\n",
    "                \n",
    "\n",
    "#                 geo_df['emb'] = geo_df['emb'].apply(lambda x: np.array2string(x, separator=',', formatter={'float_kind': lambda x: \"%.8f\" % x}, suppress_small=True, max_line_width=np.inf))\n",
    "#                 # print(geo_df['emb'][0])\n",
    "#             # Save the GeoDataFrame to the HDF5 file\n",
    "#             f.create_dataset(dataset_name, data=csv_string.encode('utf-8'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "67481825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load dataset from HDF5 file\n",
    "# def string_to_polygon(string_rep):\n",
    "#     return loads(string_rep)\n",
    "    \n",
    "# loaded_dataset = {}\n",
    "# with h5py.File('dataset.h5', 'r') as f:\n",
    "#     for column in f.keys():\n",
    "#         loaded_dataset[column] = {}\n",
    "#         for row in f[column].keys():\n",
    "#             # Retrieve the CSV string from the dataset\n",
    "#             csv_string = f[column][row][()]\n",
    "#             # Convert the CSV string back to a DataFrame\n",
    "#             # For example, you can use pandas to read the CSV string and preserve index and column labels\n",
    "#             geo_df = pd.read_csv(io.StringIO(csv_string.decode('utf-8')), index_col=0)  # Assuming the first column is the index\n",
    "#             geo_df['geometry'] = geo_df['geometry'].apply(string_to_polygon)\n",
    "#             loaded_dataset[column][row] = geo_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a4e01ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display((dataset['Kraków, Poland']['osmnx_edges']==loaded_dataset['Kraków, Poland']['osmnx_edges']).apply(pd.Series.value_counts))\n",
    "# display((dataset['Kraków, Poland']['osmnx_nodes']==loaded_dataset['Kraków, Poland']['osmnx_nodes']).apply(pd.Series.value_counts))\n",
    "# display((dataset['Kraków, Poland']['hexes']==loaded_dataset['Kraków, Poland']['hexes']).apply(pd.Series.value_counts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4252a124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def string_to_array(string_rep):\n",
    "#     # Remove brackets and split the string to get individual elements\n",
    "#     print(string_rep)\n",
    "#     elements = string_rep.strip('[]').split()\n",
    "#     print(elements)\n",
    "#     # Convert elements to floats and create a NumPy array\n",
    "#     array = np.array([float(element) for element in elements])\n",
    "\n",
    "\n",
    "#     print(array)\n",
    "#     return np.array([int(element) for element in elements])\n",
    "\n",
    "# # Apply the function to the 'emb_str' column\n",
    "# loaded_dataset['Kraków, Poland']['hexes']['emb'] = loaded_dataset['Kraków, Poland']['hexes']['emb'].apply(string_to_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c0b051c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# type(dataset['Kraków, Poland']['hexes']['emb'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3abce6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from shapely.wkt import loads\n",
    "\n",
    "# def string_to_polygon(string_rep):\n",
    "#     return loads(string_rep)\n",
    "\n",
    "# # Apply the function to the 'geometry_str' column\n",
    "# # loaded_dataset['Kraków, Poland']['hexes']['geometry'] = loaded_dataset['Kraków, Poland']['hexes']['geometry'].apply(string_to_polygon)\n",
    "# loaded_dataset['Kraków, Poland']['osmnx_edges']['geometry'] = loaded_dataset['Kraków, Poland']['osmnx_edges']['geometry'].apply(string_to_polygon)\n",
    "\n",
    "# # (dataset['Kraków, Poland']['hexes']['geometry'][0]== loaded_dataset['Kraków, Poland']['hexes']['geometry'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "255a0ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset['Kraków, Poland']['osmnx_edges']['geometry'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b123f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable\n",
    "import torch\n",
    "import gc\n",
    "from src.graph_layering.city_hetero_data import CityHeteroData\n",
    "from src.graph_layering.graph_layer_creator import GraphLayerController, SourceType\n",
    "\n",
    "\n",
    "def create_hetero_data_emb(\n",
    "    controller: GraphLayerController,\n",
    "    hexes_attrs_columns_names: Iterable[str],\n",
    "    osmnx_node_attrs_columns_names: Iterable[str],\n",
    "    osmnx_edge_attrs_columns_names: Iterable[str],\n",
    "    virtual_edge_attrs_columns_names: Iterable[str],\n",
    "    hexes_y_columns_names: Iterable[str],\n",
    "    emb:str,\n",
    "    squeeze_y: bool = True\n",
    ") -> CityHeteroData:\n",
    "    data = CityHeteroData()\n",
    "    edges_between_hexes = controller.get_edges_between_hexes()\n",
    "    edges_between_source_and_hexes = controller.get_virtual_edges_to_hexes(\n",
    "        SourceType.OSMNX_NODES\n",
    "    )\n",
    "    # print(controller.hexes_centroids_gdf)\n",
    "    osm = torch.tensor(\n",
    "        controller.hexes_centroids_gdf[hexes_attrs_columns_names].to_numpy(),\n",
    "        dtype=torch.float32,\n",
    "    )\n",
    "     \n",
    "    s = np.stack(controller.hexes_centroids_gdf[emb].values)\n",
    "\n",
    "    emb_map = torch.tensor(s, dtype=torch.float16)\n",
    "    del s\n",
    "    gc.collect()\n",
    "    data.hex.x = torch.cat((osm, emb_map), dim=1)\n",
    "    del osm, emb_map\n",
    "    gc.collect()\n",
    "    print(data.hex.x.shape)\n",
    "\n",
    "    data.hex.y = torch.tensor(\n",
    "        controller.hexes_centroids_gdf[hexes_y_columns_names].to_numpy(),\n",
    "        dtype=torch.float32,\n",
    "    ).to(torch.int64)\n",
    "\n",
    "    data.osmnx_node.x = torch.tensor(\n",
    "        controller.osmnx_nodes_gdf[osmnx_node_attrs_columns_names].to_numpy(),\n",
    "        dtype=torch.float32,\n",
    "    )\n",
    "\n",
    "    data.hex_connected_to_hex.edge_index = torch.tensor(\n",
    "        edges_between_hexes.merge(\n",
    "            controller.hexes_gdf.reset_index(),\n",
    "            left_on=\"u\",\n",
    "            right_on=\"h3_id\",\n",
    "        )\n",
    "        .rename(columns={\"region_id\": \"u_region_id\"})\n",
    "        .merge(\n",
    "            controller.hexes_gdf.reset_index(),\n",
    "            left_on=\"v\",\n",
    "            right_on=\"h3_id\",\n",
    "        )\n",
    "        .rename(columns={\"region_id\": \"v_region_id\"})[[\"u_region_id\", \"v_region_id\"]]\n",
    "        .to_numpy()\n",
    "        .T\n",
    "    )\n",
    "\n",
    "    node_to_node_connections = (\n",
    "        controller.osmnx_edges_gdf.merge(\n",
    "            controller.osmnx_nodes_gdf.reset_index(), left_on=\"u\", right_on=\"osmid\"\n",
    "        )\n",
    "        .rename(columns={\"node_id\": \"u_node_id\"})\n",
    "        .merge(controller.osmnx_nodes_gdf.reset_index(), left_on=\"v\", right_on=\"osmid\")\n",
    "        .rename(columns={\"node_id\": \"v_node_id\"})\n",
    "    )\n",
    "\n",
    "    data.osmnx_node_connected_to_osmnx_node.edge_index = torch.tensor(\n",
    "        node_to_node_connections[[\"u_node_id\", \"v_node_id\"]].to_numpy().T\n",
    "    )\n",
    "\n",
    "    data.osmnx_node_connected_to_osmnx_node.edge_attr = torch.tensor(\n",
    "        node_to_node_connections[osmnx_edge_attrs_columns_names].to_numpy(),\n",
    "        dtype=torch.float32,\n",
    "    )\n",
    "\n",
    "    data.osmnx_node_connected_to_hex.edge_index = torch.tensor(\n",
    "        edges_between_source_and_hexes[[\"source_id\", \"region_id\"]].to_numpy().T\n",
    "    )\n",
    "\n",
    "    data.osmnx_node_connected_to_hex.edge_attr = torch.tensor(\n",
    "        edges_between_source_and_hexes[virtual_edge_attrs_columns_names].to_numpy(),\n",
    "        dtype=torch.float32,\n",
    "    )\n",
    "    del node_to_node_connections\n",
    "    gc.collect()\n",
    "    if squeeze_y:\n",
    "        data.hex.y = data.hex.y.squeeze()\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f83481c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3168, 151745])\n",
      "torch.Size([3534, 151745])\n",
      "torch.Size([2945, 151745])\n"
     ]
    }
   ],
   "source": [
    "def create_torch_geometric_hetero_data(\n",
    "    osmnx_nodes, osmnx_edges, hexes, controller: GraphLayerController\n",
    "):\n",
    "    edges_attr_columns = osmnx_edges.columns[\n",
    "        ~osmnx_edges.columns.isin([\"u\", \"v\", \"key\", \"geometry\"])\n",
    "    ]\n",
    "    nodes_attr_columns = osmnx_nodes.columns[\n",
    "        ~osmnx_nodes.columns.isin([\"geometry\", \"x\", \"y\", \"osmid\", \"accidents_count\"])\n",
    "    ]\n",
    "    hexes_attr_columns = hexes.columns[~hexes.columns.isin([\"geometry\", \"h3_id\", 'emb'])]\n",
    "\n",
    "    data = create_hetero_data_emb(\n",
    "        controller,\n",
    "        hexes_attrs_columns_names=hexes_attr_columns,\n",
    "        osmnx_edge_attrs_columns_names=edges_attr_columns,\n",
    "        osmnx_node_attrs_columns_names=nodes_attr_columns,\n",
    "        virtual_edge_attrs_columns_names=[],\n",
    "        hexes_y_columns_names=[\"accident_occured\"],\n",
    "        emb='emb',\n",
    "    )\n",
    "    return data\n",
    "\n",
    "# data_dict = {}\n",
    "for city_name, gdfs in list(gdfs_dict2.items()):\n",
    "    data_dict[city_name]= create_torch_geometric_hetero_data(**gdfs)\n",
    "\n",
    "# def shift_elements_right(lst):\n",
    "#     shifted_lst = [lst[-1]] + lst[:-1]\n",
    "#     return shifted_lst\n",
    "\n",
    "\n",
    "# cities_names_list = list(data_dict.keys())\n",
    "\n",
    "# # val + test\n",
    "# folds_tuples = list(zip(shift_elements_right(cities_names_list), cities_names_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2c47aa41",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def shift_elements_right(lst):\n",
    "    shifted_lst = [lst[-1]] + lst[:-1]\n",
    "    return shifted_lst\n",
    "\n",
    "\n",
    "cities_names_list = list(data_dict.keys())\n",
    "\n",
    "# val + test\n",
    "folds_tuples = list(zip(shift_elements_right(cities_names_list), cities_names_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "54ef0906",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Poznań, Poland', 'Kraków, Poland'),\n",
       " ('Kraków, Poland', 'Warszawa, Poland'),\n",
       " ('Warszawa, Poland', 'Wrocław, Poland'),\n",
       " ('Wrocław, Poland', 'Szczecin, Poland'),\n",
       " ('Szczecin, Poland', 'Poznań, Poland')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folds_tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2656bbae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage before deletion: 2548.04296875 MB\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "for name in dir():\n",
    "    if name != \"data_dict\" and name != \"folds_tuples\" and name != \"gc\" and name != \"configs_list\" and not name.startswith('_'):\n",
    "        del globals()[name]\n",
    "\n",
    "# Call garbage collector to free up memory\n",
    "gc.collect()\n",
    "\n",
    "import psutil\n",
    "\n",
    "def memory_usage():\n",
    "    process = psutil.Process()\n",
    "    mem_info = process.memory_info()\n",
    "    return mem_info.rss / 1024 ** 2  # Return memory usage in MB\n",
    "\n",
    "print(\"Memory usage before deletion:\", memory_usage(), \"MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "205023b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "notebook_path = os.path.abspath(\"__file__\")\n",
    "notebook_directory = os.path.dirname(notebook_path)\n",
    "parent_directory = os.path.dirname(notebook_directory)\n",
    "\n",
    "parent_parent_directory = os.path.dirname(parent_directory)\n",
    "\n",
    "sys.path.append(parent_parent_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e2a13ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage before deletion: 308.0078125 MB\n"
     ]
    }
   ],
   "source": [
    "print(\"Memory usage before deletion:\", memory_usage(), \"MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "07896626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free memory before cleaning variables: 1442.79 MB\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "def get_free_memory():\n",
    "    # Get virtual memory statistics\n",
    "    mem = psutil.virtual_memory()\n",
    "    \n",
    "    # Return the amount of free memory in MB\n",
    "    return mem.available / (1024 ** 2)\n",
    "\n",
    "# Example usage:\n",
    "free_memory_before = get_free_memory()\n",
    "print(f\"Free memory before cleaning variables: {free_memory_before:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "98cf5b75",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at alloc_cpu.cpp:80] data. DefaultCPUAllocator: not enough memory: you tried to allocate 2074050660 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[63], line 132\u001b[0m\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMean F1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mmean(mean_f1s)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ± \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mstd(mean_f1s)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 132\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[63], line 107\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;66;03m# print(sweep_configs)\u001b[39;00m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, config \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(configs_list):\n\u001b[1;32m--> 107\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mrun_k_fold\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    108\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend(result)\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConfig: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[63], line 63\u001b[0m, in \u001b[0;36mrun_k_fold\u001b[1;34m(hparams)\u001b[0m\n\u001b[0;32m     57\u001b[0m val_data \u001b[38;5;241m=\u001b[39m [data_dict[val_city_name]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mclone()]\n\u001b[0;32m     58\u001b[0m train_data \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     59\u001b[0m     v\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mclone()\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m data_dict\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m val_city_name \u001b[38;5;129;01mand\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m test_city_name\n\u001b[0;32m     62\u001b[0m ]\n\u001b[1;32m---> 63\u001b[0m test_data \u001b[38;5;241m=\u001b[39m \u001b[43mdata_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtest_city_name\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m auc, accuracy, f1, model_path \u001b[38;5;241m=\u001b[39m train(\n\u001b[0;32m     66\u001b[0m     train_data\u001b[38;5;241m=\u001b[39mtrain_data,\n\u001b[0;32m     67\u001b[0m     val_data\u001b[38;5;241m=\u001b[39mval_data,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     71\u001b[0m     train_save_dir\u001b[38;5;241m=\u001b[39mTRAIN_SAVE_DIR,\n\u001b[0;32m     72\u001b[0m )\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# run.log_model(\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m#     path=model_path,\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m#     name=f\"model_{fold_group_id}_fold_{index}\",\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m# run.log({f\"accuracy_fold_{index}\": accuracy})\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# run.log({f\"f1_fold_{index}\": f1})\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Natalia\\Desktop\\gradient_new\\gradient\\.venv\\lib\\site-packages\\torch_geometric\\data\\data.py:347\u001b[0m, in \u001b[0;36mBaseData.clone\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclone\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    344\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Performs cloning of tensors, either for all attributes or only the\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;124;03m    ones given in :obj:`*args`.\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcopy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Natalia\\Desktop\\gradient_new\\gradient\\.venv\\lib\\site-packages\\torch_geometric\\data\\data.py:340\u001b[0m, in \u001b[0;36mBaseData.apply\u001b[1;34m(self, func, *args)\u001b[0m\n\u001b[0;32m    336\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Applies the function :obj:`func`, either to all attributes or only\u001b[39;00m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;124;03mthe ones given in :obj:`*args`.\u001b[39;00m\n\u001b[0;32m    338\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m store \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstores:\n\u001b[1;32m--> 340\u001b[0m     \u001b[43mstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Natalia\\Desktop\\gradient_new\\gradient\\.venv\\lib\\site-packages\\torch_geometric\\data\\storage.py:201\u001b[0m, in \u001b[0;36mBaseStorage.apply\u001b[1;34m(self, func, *args)\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Applies the function :obj:`func`, either to all attributes or only\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;124;03mthe ones given in :obj:`*args`.\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems(\u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m--> 201\u001b[0m     \u001b[38;5;28mself\u001b[39m[key] \u001b[38;5;241m=\u001b[39m \u001b[43mrecursive_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Natalia\\Desktop\\gradient_new\\gradient\\.venv\\lib\\site-packages\\torch_geometric\\data\\storage.py:895\u001b[0m, in \u001b[0;36mrecursive_apply\u001b[1;34m(data, func)\u001b[0m\n\u001b[0;32m    893\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrecursive_apply\u001b[39m(data: Any, func: Callable) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    894\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, Tensor):\n\u001b[1;32m--> 895\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    896\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mrnn\u001b[38;5;241m.\u001b[39mPackedSequence):\n\u001b[0;32m    897\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(data)\n",
      "File \u001b[1;32mc:\\Users\\Natalia\\Desktop\\gradient_new\\gradient\\.venv\\lib\\site-packages\\torch_geometric\\data\\data.py:347\u001b[0m, in \u001b[0;36mBaseData.clone.<locals>.<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclone\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    344\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Performs cloning of tensors, either for all attributes or only the\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;124;03m    ones given in :obj:`*args`.\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m copy\u001b[38;5;241m.\u001b[39mcopy(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m*\u001b[39margs)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at alloc_cpu.cpp:80] data. DefaultCPUAllocator: not enough memory: you tried to allocate 2074050660 bytes."
     ]
    }
   ],
   "source": [
    "from src.training.train import train\n",
    "import json \n",
    "# import torch\n",
    "# import psutil\n",
    "import gc\n",
    "# import copy \n",
    "# import pandas as pd\n",
    "# import geopandas as gpd\n",
    "import geopandas as gpd\n",
    "# from src.organized_datasets_creation.utils import resolve_nominatim_city_name\n",
    "# from src.graph_layering.create_dataframes import create_osmnx_dataframes\n",
    "# from src.organized_datasets_creation.utils import convert_nominatim_name_to_filename\n",
    "# from src.graph_layering.graph_layer_creator import GraphLayerController\n",
    "# import pandas as pd\n",
    "# from typing import cast\n",
    "# import os\n",
    "# from src.graph_layering.graph_layer_creator import SourceType\n",
    "# import warnings\n",
    "# from src.graph_layering.create_hetero_data import create_hetero_data\n",
    "\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# from tqdm import tqdm\n",
    "# # import wandb.util\n",
    "# # import wandb\n",
    "# import os\n",
    "\n",
    "# import glob\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def run_k_fold(hparams):\n",
    "    # run = wandb.init()\n",
    "    epochs = 5#EPOCHS\n",
    "\n",
    "    # config = wandb.config\n",
    "\n",
    "    lin_layer_sizes = [hparams['lin_layer_size']] * hparams['num_lin_layers']\n",
    "\n",
    "\n",
    "    hparams = {\n",
    "        \"hidden_channels\": hparams['hidden_channels'],\n",
    "        \"lr\": hparams['learning_rate'],\n",
    "        \"num_conv_layers\": hparams['num_conv_layers'],\n",
    "        \"lin_layer_sizes\": lin_layer_sizes,\n",
    "        \"weight_decay\": hparams['weight_decay'],\n",
    "    }\n",
    "\n",
    "    aucs = []\n",
    "    accuracies = []\n",
    "    f1s = []\n",
    "\n",
    "    # fold_group_id = generate_id()\n",
    "    TRAIN_SAVE_DIR='res'\n",
    "    for index, (val_city_name, test_city_name) in enumerate(folds_tuples):\n",
    "        val_data = [data_dict[val_city_name].to(\"cpu\").clone()]\n",
    "        train_data = [\n",
    "            v.to(\"cpu\").clone()\n",
    "            for k, v in data_dict.items()\n",
    "            if k != val_city_name and k != test_city_name\n",
    "        ]\n",
    "        test_data = data_dict[test_city_name].to(\"cpu\").clone()\n",
    "\n",
    "        auc, accuracy, f1, model_path = train(\n",
    "            train_data=train_data,\n",
    "            val_data=val_data,\n",
    "            test_data=test_data,\n",
    "            epochs=epochs,\n",
    "            hparams=hparams,\n",
    "            train_save_dir=TRAIN_SAVE_DIR,\n",
    "        )\n",
    "        # run.log_model(\n",
    "        #     path=model_path,\n",
    "        #     name=f\"model_{fold_group_id}_fold_{index}\",\n",
    "        # )\n",
    "        # run.log({f\"auc_fold_{index}\": auc})\n",
    "        # run.log({f\"accuracy_fold_{index}\": accuracy})\n",
    "        # run.log({f\"f1_fold_{index}\": f1})\n",
    "\n",
    "        aucs.append(auc)\n",
    "        accuracies.append(accuracy)\n",
    "        f1s.append(f1)\n",
    "\n",
    "    mean_auc = sum(aucs) / len(aucs)\n",
    "    mean_accuracy = sum(accuracies) / len(accuracies)\n",
    "    mean_f1 = sum(f1s) / len(f1s)\n",
    "    # run.log({\"mean_auc\": mean_auc})\n",
    "    # run.log({\"mean_accuracy\": mean_accuracy})\n",
    "    # run.log({\"mean_f1\": mean_f1})\n",
    "\n",
    "    del auc, accuracy, f1, model_path, test_data, train_data, val_data\n",
    "    gc.collect()\n",
    "    return {\n",
    "        \"mean_auc\": mean_auc,\n",
    "        \"mean_accuracy\": mean_accuracy,\n",
    "        \"mean_f1\": mean_f1,\n",
    "        \"aucs\": aucs,\n",
    "        \"accuracies\": accuracies,\n",
    "        \"f1s\": f1s,\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    results = []\n",
    "    # print(sweep_configs)\n",
    "    for i, config in enumerate(configs_list):\n",
    "        result = run_k_fold(config)\n",
    "        results.append(result)\n",
    "        print(f\"Config: {config}\")\n",
    "        print(f\"Mean AUC: {result['mean_auc']}\")\n",
    "        print(f\"Mean Accuracy: {result['mean_accuracy']}\")\n",
    "        print(f\"Mean F1: {result['mean_f1']}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        with open(f'emb_results_{i}.json', 'w') as file:\n",
    "            json.dump(result, file)\n",
    "    del result\n",
    "    gc.collect() \n",
    "    # Optionally, you can display aggregated results in a more comprehensive format\n",
    "    mean_aucs = [res[\"mean_auc\"] for res in results]\n",
    "    mean_accuracies = [res[\"mean_accuracy\"] for res in results]\n",
    "    mean_f1s = [res[\"mean_f1\"] for res in results]\n",
    "\n",
    "    print(\"Overall Results\")\n",
    "    print(f\"Mean AUC: {np.mean(mean_aucs)} ± {np.std(mean_aucs)}\")\n",
    "    print(f\"Mean Accuracy: {np.mean(mean_accuracies)} ± {np.std(mean_accuracies)}\")\n",
    "    print(f\"Mean F1: {np.mean(mean_f1s)} ± {np.std(mean_f1s)}\")\n",
    "    return results\n",
    "        \n",
    "\n",
    "\n",
    "result = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "cfa10ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 8055.59 MB\n",
      "Available: 953.99 MB\n",
      "Used: 7101.60 MB\n",
      "Free: 953.99 MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def print_memory_usage():\n",
    "    mem = psutil.virtual_memory()\n",
    "    print(f\"Total: {mem.total / (1024 ** 2):.2f} MB\")\n",
    "    print(f\"Available: {mem.available / (1024 ** 2):.2f} MB\")\n",
    "    print(f\"Used: {mem.used / (1024 ** 2):.2f} MB\")\n",
    "    print(f\"Free: {mem.free / (1024 ** 2):.2f} MB\")\n",
    "\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e860ec1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
