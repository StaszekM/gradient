{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfbe98d0",
   "metadata": {},
   "source": [
    "# 1. Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d440116b8deaa04b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T05:54:43.462692Z",
     "start_time": "2024-04-29T05:54:40.712864Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "\n",
    "import geopandas as gpd\n",
    "from src.organized_datasets_creation.utils import resolve_nominatim_city_name\n",
    "from src.organized_datasets_creation.utils import convert_nominatim_name_to_filename\n",
    "from src.graph_layering.graph_layer_creator import GraphLayerController\n",
    "import pandas as pd\n",
    "from typing import cast\n",
    "import os\n",
    "from src.graph_layering.graph_layer_creator import SourceType\n",
    "import warnings\n",
    "from src.graph_layering.create_hetero_data import create_hetero_data\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import wandb.util\n",
    "import wandb\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from src.graph.create_osmnx_graph import OSMnxGraph\n",
    "import json\n",
    "from shapely.geometry import Point\n",
    "from joblib import dump\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "from wandb.util import generate_id\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from src.training.train import train\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e274c51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4569d985",
   "metadata": {},
   "source": [
    "# 2. Setting up env variables and configs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b83fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "WANDB_API_KEY = os.environ.get(\"WANDB_API_KEY\", None)\n",
    "assert (\n",
    "    WANDB_API_KEY is not None\n",
    "), \"WANDB_API_KEY is not set, did you forget it in the config file?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe9c51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "WANDB_API_KEY = \"56fbb1bf3740faae9e7ef1917dec1f3f5426909b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7d25fb99b34b10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T05:54:43.480822Z",
     "start_time": "2024-04-29T05:54:43.463793Z"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# general settings\n",
    "ORGANIZED_HEXES_LOCATION = \"../../data/organized-hexes\"\n",
    "ORGANIZED_GRAPHS_LOCATION = \"../../data/organized_graphs\"\n",
    "OSMNX_ALL_ATTRIBUTES_LOCATION = (\n",
    "    \"../../data/osmnx_attributes.json\"\n",
    ")\n",
    "\n",
    "HEX_FI_LOCATION = (\n",
    "    \"../../data/downstream_tasks/feature_importance\"\n",
    ")\n",
    "\n",
    "# downstream task settings\n",
    "ACCIDENTS_LOCATION = \"../../data/downstream_tasks/accidents_prediction/accidents.csv\"\n",
    "TRAIN_SAVE_DIR = \"../../gradient_logs/\"\n",
    "\n",
    "SWEEP_RUNS_COUNT = 50\n",
    "EPOCHS = 300\n",
    "\n",
    "ATTRIBUTES_CONFIGURATIONS = [\n",
    "    {\n",
    "        \"USE_ORTOPHOTO\": False,\n",
    "        \"USE_HEXES_ATTRS\": {\"NUM_FEATURES\": 20, \"IN_PERCENT\": False},\n",
    "        \"USE_OSMNX_ATTRS\": True,\n",
    "    },\n",
    "    {\n",
    "        \"USE_ORTOPHOTO\": False,\n",
    "        \"USE_HEXES_ATTRS\": {\"NUM_FEATURES\": 20, \"IN_PERCENT\": True},\n",
    "        \"USE_OSMNX_ATTRS\": True,\n",
    "    },\n",
    "    {\n",
    "        \"USE_ORTOPHOTO\": False,\n",
    "        \"USE_HEXES_ATTRS\": {\"NUM_FEATURES\": 50, \"IN_PERCENT\": False},\n",
    "        \"USE_OSMNX_ATTRS\": True,\n",
    "    },\n",
    "    {\n",
    "        \"USE_ORTOPHOTO\": False,\n",
    "        \"USE_HEXES_ATTRS\": {\"NUM_FEATURES\": 50, \"IN_PERCENT\": True},\n",
    "        \"USE_OSMNX_ATTRS\": True,\n",
    "    },\n",
    "]\n",
    "\n",
    "WANDB_SWEEP_PARAMS_GRAPH_DATA = {\n",
    "    \"method\": \"bayes\",\n",
    "    \"metric\": {\"name\": \"mean_f1\", \"goal\": \"maximize\"},\n",
    "    \"parameters\": {\n",
    "        \"hidden_channels\": {\"values\": [10, 20, 30, 40, 50]},\n",
    "        \"learning_rate\": {\n",
    "            \"distribution\": \"log_uniform_values\",\n",
    "            \"min\": 1e-5,\n",
    "            \"max\": 1e-2,\n",
    "        },\n",
    "        \"num_conv_layers\": {\"values\": [1, 2, 3, 4, 5]},\n",
    "        \"lin_layer_size\": {\"values\": [8, 16, 32, 64, 128]},\n",
    "        \"num_lin_layers\": {\"values\": [0, 1, 2, 3, 4]},\n",
    "        \"weight_decay\": {\n",
    "            \"distribution\": \"log_uniform_values\",\n",
    "            \"min\": 1e-5,\n",
    "            \"max\": 1e-2,\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "WANDB_SWEEP_PARAMS_TABULAR_DATA = {\n",
    "    \"method\": \"bayes\",\n",
    "    \"metric\": {\"name\": \"mean_f1\", \"goal\": \"maximize\"},\n",
    "    \"parameters\": {\n",
    "        \"solver_penalty\": {\n",
    "            \"values\": [\n",
    "                \"lbfgs;l2\",\n",
    "                \"liblinear;l1\",\n",
    "                \"liblinear;l2\",\n",
    "                \"newton-cg;l2\",\n",
    "                \"newton-cholesky;l2\",\n",
    "                \"sag;l2\",\n",
    "                \"saga;elasticnet\",\n",
    "                \"saga;l1\",\n",
    "                \"saga;l2\",\n",
    "            ]\n",
    "        },\n",
    "        \"C\": {\n",
    "            \"distribution\": \"log_uniform_values\",\n",
    "            \"min\": 1e-5,\n",
    "            \"max\": 1,\n",
    "        },\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e887ac3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_attributes_configurations(configurations):\n",
    "    for item in configurations:\n",
    "        assert \"USE_ORTOPHOTO\" in item and isinstance(\n",
    "            item[\"USE_ORTOPHOTO\"], bool\n",
    "        ), f\"Invalid configuration: {item}, missing or invalid USE_ORTOPHOTO\"\n",
    "        assert \"USE_OSMNX_ATTRS\" in item and isinstance(\n",
    "            item[\"USE_OSMNX_ATTRS\"], bool\n",
    "        ), f\"Invalid configuration: {item}, missing or invalid USE_OSMNX_ATTRS\"\n",
    "\n",
    "        assert \"USE_HEXES_ATTRS\" in item, \"Missing USE_HEXES_ATTRS\"\n",
    "        if not isinstance(item[\"USE_HEXES_ATTRS\"], bool):\n",
    "            assert isinstance(\n",
    "                item[\"USE_HEXES_ATTRS\"], dict\n",
    "            ), \"USE_HEXES_ATTRS should be a dict\"\n",
    "            assert \"NUM_FEATURES\" in item[\"USE_HEXES_ATTRS\"], \"Missing NUM_FEATURES\"\n",
    "            assert \"IN_PERCENT\" in item[\"USE_HEXES_ATTRS\"], \"Missing IN_PERCENT\"\n",
    "            assert isinstance(\n",
    "                item[\"USE_HEXES_ATTRS\"][\"NUM_FEATURES\"], int\n",
    "            ), \"NUM_FEATURES should be an int\"\n",
    "            assert isinstance(\n",
    "                item[\"USE_HEXES_ATTRS\"][\"IN_PERCENT\"], bool\n",
    "            ), \"IN_PERCENT should be a bool\"\n",
    "\n",
    "\n",
    "verify_attributes_configurations(ATTRIBUTES_CONFIGURATIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51129540",
   "metadata": {},
   "source": [
    "# 3. Loading accidents\n",
    "\n",
    "The process includes removing unused columns and creating GeoSeries from raw X Y points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1426d7ddde7af4b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T05:54:44.922146Z",
     "start_time": "2024-04-29T05:54:43.481459Z"
    }
   },
   "outputs": [],
   "source": [
    "accidents = pd.read_csv(ACCIDENTS_LOCATION)\n",
    "\n",
    "\n",
    "def create_point(x):\n",
    "    return Point(float(x[0]), float(x[1]))\n",
    "\n",
    "\n",
    "geometry = accidents[[\"wsp_gps_x\", \"wsp_gps_y\"]].apply(create_point, axis=1)\n",
    "\n",
    "gdf_accidents = gpd.GeoDataFrame(accidents, geometry=geometry, crs=\"EPSG:4326\")\n",
    "gdf_accidents.drop(columns=[\"wsp_gps_x\", \"wsp_gps_y\", \"uczestnicy\"], inplace=True)\n",
    "gdf_accidents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ecb693",
   "metadata": {},
   "source": [
    "# 4. Displaying available cities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa453cc94b7c9a03",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T05:54:44.938471Z",
     "start_time": "2024-04-29T05:54:44.922907Z"
    }
   },
   "outputs": [],
   "source": [
    "cities = list(map(lambda x: x + \", Poland\", accidents[\"mie_nazwa\"].unique()))\n",
    "print(\"Cities:\")\n",
    "print(cities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8fa817",
   "metadata": {},
   "source": [
    "# 5. Creating GeoDataFrames\n",
    "\n",
    "The process of creation has following steps:\n",
    "\n",
    "1. loading OSMNX nodes and edges\n",
    "2. assigning accidents to OSMNX nodes\n",
    "3. taking latest H9 resolution hexes\n",
    "4. combining OSMNX nodes, OSMNX edges, hexes in a single dict and packing it inside gdfs_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f1124060ba1239",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T05:57:59.983530Z",
     "start_time": "2024-04-29T05:54:44.939172Z"
    }
   },
   "outputs": [],
   "source": [
    "def add_accidents_to_osmnx_nodes(\n",
    "    accidents: gpd.GeoDataFrame,\n",
    "    nodes: gpd.GeoDataFrame,\n",
    "    edges: gpd.GeoDataFrame,\n",
    "    city_name: str,\n",
    "):\n",
    "    with open(OSMNX_ALL_ATTRIBUTES_LOCATION) as f:\n",
    "        all_attributes = json.load(f)\n",
    "\n",
    "    osmnx_graph = OSMnxGraph(\n",
    "        accidents.loc[\n",
    "            accidents[\"mie_nazwa\"] == resolve_nominatim_city_name(city_name), :\n",
    "        ],\n",
    "        nodes,\n",
    "        edges,\n",
    "        all_attributes,\n",
    "    )\n",
    "    osmnx_graph._aggregate(element_type=\"node\", aggregation_method=\"count\")\n",
    "    return osmnx_graph.gdf_nodes\n",
    "\n",
    "\n",
    "def create_gdfs(city_name: str, accidents_gdf: gpd.GeoDataFrame = gdf_accidents):\n",
    "    city_folder_name = convert_nominatim_name_to_filename(\n",
    "        resolve_nominatim_city_name(city_name)\n",
    "    )\n",
    "    osmnx_nodes = gpd.read_parquet(\n",
    "        os.path.join(ORGANIZED_GRAPHS_LOCATION, city_folder_name, \"nodes.parquet\")\n",
    "    )\n",
    "    osmnx_nodes = osmnx_nodes.reset_index()\n",
    "    osmnx_nodes.index.names = [\"node_id\"]\n",
    "    osmnx_nodes[\"x\"] = osmnx_nodes[\"geometry\"].x\n",
    "    osmnx_nodes[\"y\"] = osmnx_nodes[\"geometry\"].y\n",
    "\n",
    "    osmnx_edges = gpd.read_parquet(\n",
    "        os.path.join(ORGANIZED_GRAPHS_LOCATION, city_folder_name, \"edges.parquet\")\n",
    "    )\n",
    "    osmnx_edges = osmnx_edges.reset_index().rename(columns={\"index\": \"edge_id\"})\n",
    "    osmnx_edges.index.names = [\"edge_id\"]\n",
    "\n",
    "    assert osmnx_nodes.crs == osmnx_edges.crs\n",
    "    assert osmnx_nodes.crs == accidents_gdf.crs\n",
    "\n",
    "    osmnx_nodes = add_accidents_to_osmnx_nodes(\n",
    "        accidents=accidents_gdf,\n",
    "        nodes=osmnx_nodes,\n",
    "        city_name=city_name,\n",
    "        edges=osmnx_edges,\n",
    "    )\n",
    "\n",
    "    hexes_years_folder = os.path.join(ORGANIZED_HEXES_LOCATION, city_folder_name)\n",
    "\n",
    "    subfolders = [\n",
    "        int(f)\n",
    "        for f in os.listdir(hexes_years_folder)\n",
    "        if os.path.isdir(os.path.join(hexes_years_folder, f))\n",
    "    ]\n",
    "    highest_year = subfolders[np.argmax(subfolders)]\n",
    "\n",
    "    hexes: gpd.GeoDataFrame = gpd.read_parquet(\n",
    "        os.path.join(\n",
    "            ORGANIZED_HEXES_LOCATION,\n",
    "            f\"{convert_nominatim_name_to_filename(resolve_nominatim_city_name(city_name))}/{highest_year}/h9/count-embedder/dataset.parquet\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    hexes = hexes.rename(columns={\"region_id\": \"h3_id\"}).rename_axis(\n",
    "        \"region_id\", axis=0\n",
    "    )\n",
    "\n",
    "    return dict(osmnx_nodes=osmnx_nodes, osmnx_edges=osmnx_edges, hexes=hexes)\n",
    "\n",
    "\n",
    "print(\"Creating gdfs...\")\n",
    "gdfs_dict = {city_name: create_gdfs(city_name) for city_name in tqdm(cities)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6304be2",
   "metadata": {},
   "source": [
    "# 6. Creating GraphLayerController for each of the cities\n",
    "\n",
    "The creation is based on previously made GeoDataFrames. The controller is used to transfer accidents Y values from OSMNX nodes to hexes. It is also used to create complete graph data in case of graph datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdc8136326251d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T05:58:00.358809Z",
     "start_time": "2024-04-29T05:58:00.142909Z"
    }
   },
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    for gdf_for_city in gdfs_dict.values():\n",
    "        gdf_for_city[\"controller\"] = GraphLayerController(\n",
    "            gdf_for_city[\"hexes\"],\n",
    "            gdf_for_city[\"osmnx_nodes\"],\n",
    "            gdf_for_city[\"osmnx_edges\"],\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402c77ee",
   "metadata": {},
   "source": [
    "# 7. Patching hexes\n",
    "\n",
    "The y value (1 = accident occured, 0 = no accident) is assigned to each of the hexes according to its underlying OSMNX nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0aac7a866a12b15",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T05:58:00.376920Z",
     "start_time": "2024-04-29T05:58:00.359616Z"
    }
   },
   "outputs": [],
   "source": [
    "def patch_hexes_with_y(\n",
    "    osmnx_nodes: gpd.GeoDataFrame,\n",
    "    hexes: gpd.GeoDataFrame,\n",
    "    controller: GraphLayerController,\n",
    "):\n",
    "    virtual_edges = controller.get_virtual_edges_to_hexes(SourceType.OSMNX_NODES)\n",
    "    hexes_with_y = cast(\n",
    "        gpd.GeoDataFrame,\n",
    "        hexes.merge(\n",
    "            virtual_edges.merge(osmnx_nodes, left_on=\"source_id\", right_index=True)[\n",
    "                [\"region_id\", \"accidents_count\"]\n",
    "            ]\n",
    "            .groupby(\"region_id\")\n",
    "            .sum(),\n",
    "            left_index=True,\n",
    "            right_index=True,\n",
    "            how=\"left\",\n",
    "        ).fillna(0),\n",
    "    )\n",
    "    hexes_with_y[\"accident_occured\"] = (hexes_with_y[\"accidents_count\"] > 0).astype(int)\n",
    "    hexes_with_y.drop(columns=\"accidents_count\", inplace=True)\n",
    "    controller.hexes_gdf = hexes_with_y\n",
    "    controller._hexes_centroids_gdf = controller._create_hexes_centroids_gdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7348370ea8cdaece",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T05:58:00.689761Z",
     "start_time": "2024-04-29T05:58:00.377730Z"
    }
   },
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    for gdfs in gdfs_dict.values():\n",
    "        patch_hexes_with_y(gdfs[\"osmnx_nodes\"], gdfs[\"hexes\"], gdfs[\"controller\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3025c1ec",
   "metadata": {},
   "source": [
    "# 8. Creating graph data\n",
    "\n",
    "Graph data is used when we include OSMNX attributes and in turn maintain the graph structure of the data\n",
    "\n",
    "The data is created only once for now just to create (train, val, test) folds labels for crossvalidation on graph-based versions of the task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa099bf6c66b97cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T05:58:01.878441Z",
     "start_time": "2024-04-29T05:58:00.690710Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import List, Literal, Union\n",
    "\n",
    "\n",
    "def create_graph_data(\n",
    "    osmnx_nodes,\n",
    "    osmnx_edges,\n",
    "    hexes,\n",
    "    controller: GraphLayerController,\n",
    "    use_hexes_attr: bool,\n",
    "    use_ortophoto: bool,\n",
    "    columns_to_take: Union[List[str], Literal[\"all\"]] = [],\n",
    "):\n",
    "\n",
    "    edges_attr_columns = osmnx_edges.columns[\n",
    "        ~osmnx_edges.columns.isin([\"u\", \"v\", \"key\", \"geometry\"])\n",
    "    ]\n",
    "    nodes_attr_columns = osmnx_nodes.columns[\n",
    "        ~osmnx_nodes.columns.isin([\"geometry\", \"x\", \"y\", \"osmid\"])\n",
    "    ]\n",
    "\n",
    "    if use_hexes_attr:\n",
    "        hexes_attr_columns = (\n",
    "            hexes.columns[~hexes.columns.isin([\"geometry\", \"h3_id\", \"price_class\"])]\n",
    "            if columns_to_take == \"all\"\n",
    "            else columns_to_take\n",
    "        )\n",
    "    else:\n",
    "        hexes_attr_columns = []\n",
    "\n",
    "    data = create_hetero_data(\n",
    "        controller,\n",
    "        hexes_attrs_columns_names=hexes_attr_columns,\n",
    "        osmnx_edge_attrs_columns_names=edges_attr_columns,\n",
    "        osmnx_node_attrs_columns_names=nodes_attr_columns,\n",
    "        virtual_edge_attrs_columns_names=[],\n",
    "        hexes_y_columns_names=[\"accident_occured\"],\n",
    "    )\n",
    "    return data\n",
    "\n",
    "\n",
    "graph_data_dict = {\n",
    "    city_name: create_graph_data(**gdfs, use_ortophoto=True, use_hexes_attr=True)\n",
    "    for city_name, gdfs in gdfs_dict.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa7d7c7",
   "metadata": {},
   "source": [
    "# 9. Creating tabular data\n",
    "\n",
    "Tabular data is used when we omit OSMNX attributes and in turn lose the graph structure of the data\n",
    "\n",
    "No folds creation on tabular-based versions of the task - using simple leave-one-out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d17b304",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tabular_data(\n",
    "    hexes: pd.DataFrame,\n",
    "    controller: GraphLayerController,\n",
    "    use_hexes_attr: bool,\n",
    "    use_ortophoto: bool,\n",
    "):\n",
    "    assert use_ortophoto or use_hexes_attr, \"Provide at least one data source\"\n",
    "\n",
    "    hexes_attr_columns = (\n",
    "        hexes.columns[~hexes.columns.isin([\"geometry\", \"h3_id\"])]\n",
    "        if use_hexes_attr\n",
    "        else []\n",
    "    )\n",
    "\n",
    "    hexes_y_columns_names = [\"accident_occured\"]\n",
    "\n",
    "    X = hexes[hexes_attr_columns]\n",
    "    y = controller.hexes_centroids_gdf[hexes_y_columns_names]\n",
    "\n",
    "    return {\"X\": X, \"y\": y}\n",
    "\n",
    "\n",
    "tabular_data_dict = {\n",
    "    city_name: create_tabular_data(\n",
    "        gdfs[\"hexes\"],\n",
    "        cast(GraphLayerController, gdfs[\"controller\"]),\n",
    "        use_ortophoto=False,\n",
    "        use_hexes_attr=True,\n",
    "    )\n",
    "    for city_name, gdfs in gdfs_dict.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da44b71",
   "metadata": {},
   "source": [
    "# 10. Creating folds labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735adf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_elements_right(lst):\n",
    "    shifted_lst = [lst[-1]] + lst[:-1]\n",
    "    return shifted_lst\n",
    "\n",
    "\n",
    "cities_names_list = list(graph_data_dict.keys())\n",
    "cities_names_list.sort(key=lambda x: str(x))\n",
    "\n",
    "# val + test\n",
    "folds_tuples = list(zip(shift_elements_right(cities_names_list), cities_names_list))\n",
    "display(folds_tuples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd185f7",
   "metadata": {},
   "source": [
    "# 11. Functions setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312d98f295cfeb0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T07:59:07.807822Z",
     "start_time": "2024-04-29T07:58:54.359959Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_k_fold_graph_data(closure_config, sweep_id):\n",
    "    # pass external config (i.e. what attributes are used in the data), closure to avoid passing it to the function directly\n",
    "    def wrapped():\n",
    "        run = wandb.init()\n",
    "        epochs = EPOCHS\n",
    "\n",
    "        config = wandb.config\n",
    "\n",
    "        for k, v in closure_config.items():\n",
    "            run.log({k: 1 if v else 0})\n",
    "\n",
    "        run.log({\"data_structure\": \"graph\"})\n",
    "\n",
    "        # create hparams\n",
    "        if hasattr(config, \"lin_layer_size\") and hasattr(config, \"num_lin_layers\"):\n",
    "            lin_layer_sizes = [config.lin_layer_size] * config.num_lin_layers\n",
    "        else:\n",
    "            lin_layer_sizes = config.lin_layer_sizes\n",
    "        hparams = {\n",
    "            \"hidden_channels\": config.hidden_channels,\n",
    "            \"lr\": config.learning_rate,\n",
    "            \"num_conv_layers\": config.num_conv_layers,\n",
    "            \"lin_layer_sizes\": lin_layer_sizes,\n",
    "            \"weight_decay\": config.weight_decay,\n",
    "        }\n",
    "\n",
    "        aucs = []\n",
    "        accuracies = []\n",
    "        f1s = []\n",
    "\n",
    "        fold_group_id = generate_id()\n",
    "\n",
    "        # log data as artifact if no data was logged in the sweep before\n",
    "        # dataset is uploaded only on the first run in sweep, because it does not change across runs in sweep\n",
    "        # in wandb, dataset will be visible on the first run in the sweep\n",
    "        artifact_path = os.path.join(TRAIN_SAVE_DIR, f\"graph_data_{sweep_id}.pkl\")\n",
    "        if not os.path.exists(artifact_path):\n",
    "            dump(\n",
    "                graph_data_dict,\n",
    "                artifact_path,\n",
    "                protocol=5,\n",
    "            )\n",
    "            artifact = wandb.Artifact(\n",
    "                name=\"graph_data\", type=\"dataset\", metadata=closure_config\n",
    "            )\n",
    "            artifact.add_file(local_path=artifact_path)\n",
    "            run.log_artifact(artifact)\n",
    "\n",
    "        # run k-fold\n",
    "        for index, (val_city_name, test_city_name) in enumerate(folds_tuples):\n",
    "            # prepare data\n",
    "            val_data = [graph_data_dict[val_city_name].to(\"cpu\").clone()]\n",
    "            train_data = [\n",
    "                v.to(\"cpu\").clone()\n",
    "                for k, v in graph_data_dict.items()\n",
    "                if k != val_city_name and k != test_city_name\n",
    "            ]\n",
    "            test_data = graph_data_dict[test_city_name].to(\"cpu\").clone()\n",
    "\n",
    "            # run training with checkpointing on lowest val_loss, return test metrics for the best model and its path\n",
    "            # builtin preprocessing - scaling to N(0, 1)\n",
    "            auc, accuracy, f1, model_path = train(\n",
    "                train_data=train_data,\n",
    "                val_data=val_data,\n",
    "                test_data=test_data,\n",
    "                epochs=epochs,\n",
    "                hparams=hparams,\n",
    "                train_save_dir=TRAIN_SAVE_DIR,\n",
    "                num_classes=2,\n",
    "            )\n",
    "\n",
    "            # logging - single fold\n",
    "            run.log_model(\n",
    "                path=model_path,\n",
    "                name=f\"model_{fold_group_id}_fold_{index}\",\n",
    "            )\n",
    "            run.log({f\"auc_fold_{index}\": auc})\n",
    "            run.log({f\"accuracy_fold_{index}\": accuracy})\n",
    "            run.log({f\"f1_fold_{index}\": f1})\n",
    "\n",
    "            aucs.append(auc)\n",
    "            accuracies.append(accuracy)\n",
    "            f1s.append(f1)\n",
    "\n",
    "        # logging - summary statistics\n",
    "        mean_auc = sum(aucs) / len(aucs)\n",
    "        mean_accuracy = sum(accuracies) / len(accuracies)\n",
    "        mean_f1 = sum(f1s) / len(f1s)\n",
    "        run.log({\"mean_auc\": mean_auc})\n",
    "        run.log({\"mean_accuracy\": mean_accuracy})\n",
    "        run.log({\"mean_f1\": mean_f1})\n",
    "\n",
    "    return wrapped\n",
    "\n",
    "\n",
    "def run_k_fold_tabular_data(closure_config, sweep_id):\n",
    "    # analogously to the graph data, but for tabular data\n",
    "    def wrapped():\n",
    "        run = wandb.init()\n",
    "\n",
    "        config = wandb.config\n",
    "\n",
    "        for k, v in closure_config.items():\n",
    "            run.log({k: 1 if v else 0})\n",
    "\n",
    "        run.log({\"data_structure\": \"tabular\"})\n",
    "\n",
    "        hparams = {}\n",
    "        hparams[\"C\"] = config[\"C\"]\n",
    "        solver, penalty = config[\"solver_penalty\"].split(\";\")\n",
    "        hparams[\"solver\"] = solver\n",
    "        if penalty == \"None\":\n",
    "            penalty = None\n",
    "        hparams[\"penalty\"] = penalty\n",
    "\n",
    "        aucs = []\n",
    "        accuracies = []\n",
    "        f1s = []\n",
    "\n",
    "        fold_group_id = generate_id()\n",
    "\n",
    "        # log data as artifact\n",
    "        artifact_path = os.path.join(TRAIN_SAVE_DIR, f\"tabular_data_{sweep_id}.pkl\")\n",
    "\n",
    "        if not os.path.exists(artifact_path):\n",
    "            dump(\n",
    "                tabular_data_dict,\n",
    "                artifact_path,\n",
    "                protocol=5,\n",
    "            )\n",
    "            artifact = wandb.Artifact(\n",
    "                name=\"tabular_data\", type=\"dataset\", metadata=closure_config\n",
    "            )\n",
    "            artifact.add_file(local_path=artifact_path)\n",
    "            run.log_artifact(artifact)\n",
    "\n",
    "        timestamp = datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "\n",
    "        for index, test_city_name in enumerate(cities_names_list):\n",
    "            scaler = StandardScaler()\n",
    "            X = pd.concat(\n",
    "                [\n",
    "                    m[\"X\"]\n",
    "                    for key, m in tabular_data_dict.items()\n",
    "                    if key != test_city_name\n",
    "                ]\n",
    "            ).to_numpy()\n",
    "            y = (\n",
    "                pd.concat(\n",
    "                    [\n",
    "                        m[\"y\"]\n",
    "                        for key, m in tabular_data_dict.items()\n",
    "                        if key != test_city_name\n",
    "                    ]\n",
    "                )\n",
    "                .to_numpy()\n",
    "                .ravel()\n",
    "            )\n",
    "\n",
    "            X = scaler.fit_transform(X)\n",
    "\n",
    "            logistic_regression = LogisticRegression(\n",
    "                C=hparams[\"C\"],\n",
    "                solver=hparams[\"solver\"],\n",
    "                penalty=hparams[\"penalty\"],\n",
    "                dual=False,\n",
    "                tol=1e-4,\n",
    "                fit_intercept=True,\n",
    "                intercept_scaling=1,\n",
    "                class_weight=\"balanced\",\n",
    "                random_state=1124,\n",
    "                max_iter=1000,\n",
    "                multi_class=\"auto\",\n",
    "                warm_start=False,\n",
    "                n_jobs=-1,\n",
    "                l1_ratio=0.5,\n",
    "            )\n",
    "            logistic_regression.fit(X, y)\n",
    "\n",
    "            test_X = tabular_data_dict[test_city_name][\"X\"].to_numpy()\n",
    "            test_X = scaler.transform(test_X)\n",
    "            test_y = tabular_data_dict[test_city_name][\"y\"].to_numpy().ravel()\n",
    "            y_pred = logistic_regression.predict(test_X)\n",
    "            y_proba = logistic_regression.predict_proba(test_X)[:, 1]\n",
    "\n",
    "            auc = roc_auc_score(test_y, y_proba, average=\"micro\")\n",
    "            accuracy = (y_pred == test_y).mean()\n",
    "            f1 = f1_score(\n",
    "                test_y,\n",
    "                y_pred,\n",
    "                pos_label=1,\n",
    "                average=\"binary\",\n",
    "            )\n",
    "\n",
    "            model_dir = os.path.join(TRAIN_SAVE_DIR, timestamp)\n",
    "\n",
    "            os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "            model_path = os.path.join(\n",
    "                model_dir, f\"model_{fold_group_id}_fold_{index}.pkl\"\n",
    "            )\n",
    "\n",
    "            with open(model_path, \"wb\") as f:\n",
    "                dump(logistic_regression, f, protocol=5)\n",
    "\n",
    "            run.log_model(\n",
    "                path=model_path,\n",
    "                name=f\"model_{fold_group_id}_fold_{index}\",\n",
    "            )\n",
    "            run.log({f\"auc_fold_{index}\": auc})\n",
    "            run.log({f\"accuracy_fold_{index}\": accuracy})\n",
    "            run.log({f\"f1_fold_{index}\": f1})\n",
    "\n",
    "            aucs.append(auc)\n",
    "            accuracies.append(accuracy)\n",
    "            f1s.append(f1)\n",
    "\n",
    "        mean_auc = sum(aucs) / len(aucs)\n",
    "        mean_accuracy = sum(accuracies) / len(accuracies)\n",
    "        mean_f1 = sum(f1s) / len(f1s)\n",
    "        run.log({\"mean_auc\": mean_auc})\n",
    "        run.log({\"mean_accuracy\": mean_accuracy})\n",
    "        run.log({\"mean_f1\": mean_f1})\n",
    "\n",
    "    return wrapped\n",
    "\n",
    "\n",
    "def run_sweep_graph_data(config):\n",
    "    try:\n",
    "        wandb.login(key=WANDB_API_KEY)\n",
    "        sweep_id = wandb.sweep(\n",
    "            WANDB_SWEEP_PARAMS_GRAPH_DATA, project=\"accidents-downstream-task-v2\"\n",
    "        )\n",
    "\n",
    "        wandb.agent(\n",
    "            sweep_id,\n",
    "            function=run_k_fold_graph_data(config, sweep_id),\n",
    "            count=SWEEP_RUNS_COUNT,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        wandb.finish()\n",
    "        wandb.sweep\n",
    "        raise e\n",
    "\n",
    "\n",
    "def run_sweep_tabular_data(config):\n",
    "    try:\n",
    "        wandb.login(key=WANDB_API_KEY)\n",
    "\n",
    "        sweep_id = wandb.sweep(\n",
    "            WANDB_SWEEP_PARAMS_TABULAR_DATA, project=\"accidents-downstream-task-v2\"\n",
    "        )\n",
    "\n",
    "        wandb.agent(\n",
    "            sweep_id,\n",
    "            function=run_k_fold_tabular_data(config, sweep_id),\n",
    "            count=SWEEP_RUNS_COUNT,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        wandb.finish()\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983e8cf8",
   "metadata": {},
   "source": [
    "# 12. Run functions\n",
    "\n",
    "For each config:\n",
    "\n",
    "1. Determine if config requires tabular or graph data\n",
    "2. Create data excluding attributes not included in the config\n",
    "3. Run the sweep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa3792a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef83315",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict\n",
    "\n",
    "def derive_data_structure(attr_config):\n",
    "    if attr_config[\"USE_OSMNX_ATTRS\"]:\n",
    "        return \"graph\"\n",
    "    return \"tabular\"\n",
    "\n",
    "\n",
    "configs_size = len(ATTRIBUTES_CONFIGURATIONS)\n",
    "\n",
    "for index, attr_config in enumerate(ATTRIBUTES_CONFIGURATIONS):\n",
    "    print(\"Sweep for config {}/{} in progress...\".format(index + 1, configs_size))\n",
    "\n",
    "    # assert \"USE_ORTOPHOTO\" in attr_config, \"Provide USE_ORTOPHOTO key\"\n",
    "    # assert \"USE_HEXES_ATTRS\" in attr_config, \"Provide USE_HEXES_ATTRS key\"\n",
    "    # assert \"USE_OSMNX_ATTRS\" in attr_config, \"Provide USE_OSMNX_ATTRS key\"\n",
    "\n",
    "    data_structure = derive_data_structure(attr_config)\n",
    "    \n",
    "    creator_params: Dict[str, Any] = dict(\n",
    "        use_hexes_attr=bool(attr_config[\"USE_HEXES_ATTRS\"]),\n",
    "    )\n",
    "    \n",
    "    \n",
    "    if isinstance(attr_config[\"USE_HEXES_ATTRS\"], dict):\n",
    "        hex_fi_config = attr_config[\"USE_HEXES_ATTRS\"]\n",
    "        hex_features = pd.read_json(\n",
    "            f\"{HEX_FI_LOCATION}/accidents_top_{hex_fi_config['NUM_FEATURES']}_percent_{hex_fi_config['IN_PERCENT']}.json\"\n",
    "        )\n",
    "        hex_features = hex_features[\"top_values\"].tolist()\n",
    "        creator_params[\"columns_to_take\"] = hex_features\n",
    "    elif attr_config[\"USE_HEXES_ATTRS\"] == True:\n",
    "        creator_params[\"columns_to_take\"] = \"all\"    \n",
    "    \n",
    "    if data_structure == \"graph\":\n",
    "        graph_data_dict = {\n",
    "            city_name: create_graph_data(\n",
    "                hexes=gdfs[\"hexes\"],\n",
    "                controller=cast(GraphLayerController, gdfs[\"controller\"]),\n",
    "                osmnx_edges=gdfs[\"osmnx_edges\"],\n",
    "                osmnx_nodes=gdfs[\"osmnx_nodes\"],\n",
    "                use_ortophoto=attr_config[\"USE_ORTOPHOTO\"],\n",
    "                **creator_params,\n",
    "            )\n",
    "            for city_name, gdfs in gdfs_dict.items()\n",
    "        }\n",
    "        run_sweep_graph_data(attr_config)\n",
    "    elif data_structure == \"tabular\":\n",
    "        tabular_data_dict = {\n",
    "            city_name: create_tabular_data(\n",
    "                hexes=gdfs[\"hexes\"],\n",
    "                controller=cast(GraphLayerController, gdfs[\"controller\"]),\n",
    "                use_ortophoto=attr_config[\"USE_ORTOPHOTO\"],\n",
    "                **creator_params,\n",
    "            )\n",
    "            for city_name, gdfs in gdfs_dict.items()\n",
    "        }\n",
    "        run_sweep_tabular_data(attr_config)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown data structure\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
