{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Dokumentacja Ray Tune](https://docs.ray.io/en/latest/tune/key-concepts.html)\n",
    "\n",
    "# Wstęp\n",
    "\n",
    "Notebook zaczniemy od załadowania prostego problemu do wyuczenia, tj. trening sieci konwolucyjnej na CIFAR10. Wytrenujemy sieć w podstawowy sposób, a następnie spróbujemy skorzystać z Ray Tune, by proces uczenia popchnąć w stronę lepszych wyników modelu.\n",
    "\n",
    "W trakcie przygotowań zauważymy sporo hiperparametrów uczenia, które będziemy próbowali optymalizować z pomocą Ray Tune. Hiperparametry te dla widoczności będziemy zapisywać wielkimi literami z przedrostkiem `HP`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prosty problem do wyuczenia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "import ray\n",
    "from ray import train\n",
    "import ray.cloudpickle as pickle\n",
    "import os, tempfile\n",
    "from ray.train import Checkpoint\n",
    "from ray.air import ScalingConfig\n",
    "\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5\n",
    "    npimg = img.detach().cpu().numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformacje, DataLoadery, etykiety klas\n",
    "\n",
    "Zacznijmy od przygotowania zbiorów danych uczących i testowych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    ")\n",
    "\n",
    "HP_BATCH_SIZE = 4\n",
    "\n",
    "dataset_location = \"./data\"\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root=dataset_location, train=True, download=True, transform=transform\n",
    ")\n",
    "trainloader = DataLoader(\n",
    "    trainset, batch_size=HP_BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True\n",
    ")\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root=dataset_location, train=False, download=True, transform=transform\n",
    ")\n",
    "testloader = DataLoader(\n",
    "    testset, batch_size=HP_BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True\n",
    ")\n",
    "\n",
    "classes = (\n",
    "    \"plane\",\n",
    "    \"car\",\n",
    "    \"bird\",\n",
    "    \"cat\",\n",
    "    \"deer\",\n",
    "    \"dog\",\n",
    "    \"frog\",\n",
    "    \"horse\",\n",
    "    \"ship\",\n",
    "    \"truck\",\n",
    ")\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trenowana sieć neuronowa\n",
    "\n",
    "Przygotujmy sieć o nazwie `Net`, która jest prostą siecią konwolucyjną użytą w naszym zadaniu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "source": [
    "HP_NET_OUT_CHANNELS_1 = 6\n",
    "HP_NET_KERNEL_SIZE = 5\n",
    "HP_NET_OUT_CHANNELS_2 = 16\n",
    "HP_NET_LINEAR_SIZE_1 = 120\n",
    "HP_NET_LINEAR_SIZE_2 = 84\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(\n",
    "        self, out_channels_1, out_channels_2, kernel_size, linear_size_1, linear_size_2\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, out_channels_1, kernel_size)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(out_channels_1, out_channels_2, kernel_size)\n",
    "        self.flatten = nn.Flatten(1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            fc1_size = self.flatten(\n",
    "                self.pool(self.conv2(self.pool(self.conv1(torch.ones(1, 3, 32, 32)))))\n",
    "            ).shape[1]\n",
    "\n",
    "        self.fc1 = nn.Linear(fc1_size, linear_size_1)\n",
    "        self.fc2 = nn.Linear(linear_size_1, linear_size_2)\n",
    "        self.fc3 = nn.Linear(linear_size_2, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.flatten(x)  # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "net = Net(\n",
    "    HP_NET_OUT_CHANNELS_1,\n",
    "    HP_NET_OUT_CHANNELS_2,\n",
    "    HP_NET_KERNEL_SIZE,\n",
    "    HP_NET_LINEAR_SIZE_1,\n",
    "    HP_NET_LINEAR_SIZE_2,\n",
    ").to(device)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funkcja błędu, optymalizator SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "source": [
    "HP_SGD_LEARNING_RATE = 0.001\n",
    "HP_SGD_MOMENTUM = 0.9\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "source": [
    "for epoch in range(1):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs.to(device))\n",
    "        loss = criterion(outputs, labels.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:  # print every 2000 mini-batches\n",
    "            print(f\"[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "print(\"Finished Training\")\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zapis wytrenowanego modelu do pliku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "source": [
    "PATH = \"./data/tune-example/cifar_net.pth\"\n",
    "torch.save(net.state_dict(), PATH)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ocena wyników"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "source": [
    "dataiter = iter(testloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# print images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print(\"Ground truth: \", \" \".join(f\"{classes[labels[j]]:5s}\" for j in range(4)))\n",
    "\n",
    "best_net = Net(\n",
    "    HP_NET_OUT_CHANNELS_1,\n",
    "    HP_NET_OUT_CHANNELS_2,\n",
    "    HP_NET_KERNEL_SIZE,\n",
    "    HP_NET_LINEAR_SIZE_1,\n",
    "    HP_NET_LINEAR_SIZE_2,\n",
    ")\n",
    "best_net.load_state_dict(torch.load(PATH))\n",
    "outputs = best_net(images)\n",
    "\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "print(\"Predicted: \", \" \".join(f\"{classes[predicted[j]]:5s}\" for j in range(4)))\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "source": [
    "preds = []\n",
    "ground_truth = []\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = best_net(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        preds.append(predicted)\n",
    "        ground_truth.append(labels)\n",
    "\n",
    "\n",
    "preds = torch.hstack(preds)\n",
    "ground_truth = torch.hstack(ground_truth)\n",
    "\n",
    "print(\n",
    "    f\"Accuracy of the network on the 10000 test images: {accuracy_score(ground_truth, preds)} %\"\n",
    ")\n",
    "print(\n",
    "    f\"F1 macro of the network on the 10000 test images: {f1_score(ground_truth, preds, average='macro')} %\"\n",
    ")\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Podsumowanie pierwszej części\n",
    "\n",
    "Model jest nieco lepszy (F1 score ~$50\\%$) niż losowy wybór, jednak czy da się poprawić jego wynik? Spróbujmy nasz trening ulepszyć z pomocą Ray Tune."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ray Tune\n",
    "\n",
    "Ray Tune jest biblioteką służącą do rozproszonego uruchamiania eksperymentów ML i optymalizacji ich hiperparametrów. Wspiera różne biblioteki uczenia maszynowego (m.in. TensorFlow, PyTorch) oraz integruje się z innymi narzędziami optymalizacji hiperparametrów (np. Optuna, BayesOpt).\n",
    "\n",
    "[Dokumentacja](https://docs.ray.io/en/latest/tune/key-concepts.html#key-concepts-of-ray-tune)\n",
    "\n",
    "## Podstawowe pojęcia\n",
    "\n",
    "- `search space` - przestrzeń poszukiwań wartości hiperparametrów, tj. jakie hiperparametry i jakie wartości dla nich\n",
    "- `trainable` - optymalizowana funkcja celu. \n",
    "- `search algorithm` - metoda poszukiwania wartości hiperparametrów w przestrzeni\n",
    "- `scheduler` - opcjonalny algorytm zatrzymujący nieobiecujące przeszukiwania\n",
    "- `Tuner` - instancja eksperymentu, która gromadzi search space, trainable, search algorithm i scheduler \n",
    "- `trial` - pojedyncze uruchomienie eksperymentu w wybranej konfiguracji."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search space\n",
    "\n",
    "Skonstruujmy przestrzeń poszukiwań dla naszego problemu. Ray dostarcza dużo różnych metod tworzenia przestrzeni dla każdego hiperparametru osobno, wśród których warto wyróżnić `loguniform`, która pozwala na łatwe przeszukiwanie wartości o różnych rzędach wielkości.\n",
    "\n",
    "[Link do innych przestrzeni](https://docs.ray.io/en/latest/tune/api/search_space.html#tune-search-space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "source": [
    "config = {\n",
    "    \"HP_NET_OUT_CHANNELS_1\": tune.lograndint(1, 100),\n",
    "    \"HP_NET_OUT_CHANNELS_2\": tune.lograndint(1, 100),\n",
    "    \"HP_NET_KERNEL_SIZE\": tune.randint(2, 6),\n",
    "    \"HP_NET_LINEAR_SIZE_1\": tune.lograndint(1, 100),\n",
    "    \"HP_NET_LINEAR_SIZE_2\": tune.lograndint(1, 100),\n",
    "    \"HP_BATCH_SIZE\": tune.choice([2, 4, 8, 16, 32, 64, 128, 256, 512]),\n",
    "    \"HP_SGD_LEARNING_RATE\": tune.loguniform(1e-5, 1e-1),\n",
    "    \"HP_SGD_MOMENTUM\": tune.loguniform(1e-5, 1e-1),\n",
    "}\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainable\n",
    "\n",
    "Trainable to nic innego jak funkcja uczenia, która dodatkowo posiada możliwość przekazania do procesu Ray statystyk uczenia w trakcie swojego działania. Dzięki temu Ray może kontrolować, które procesy dają szansę na wysoki wynik, a które nie.\n",
    "\n",
    "Trainable jest funkcją, która otrzymuje konfigurację danego eksperymentu jako `config`. Zawiera on m.in. hiperparametry w eksperymencie. Jest to zwykły `dict`.\n",
    "\n",
    "Nasz trainable to będzie funkcja przygotowująca dataloadery + model + uczenie w jednym. Każdy z tych etapów potrzebuje jakichś hiperparametrów, stąd takie połączenie.\n",
    "\n",
    "W kodzie można też zauważyć miejsca na checkpointy. Ray sam dba o to, by zapisywać/ładować trialsy z checkpointów.\n",
    "\n",
    "[Link do API trainable](https://docs.ray.io/en/latest/tune/api/trainable.html#function-trainable-api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "source": [
    "dataset_location = \"/home/stachu/MGR/projekt/data\"\n",
    "accuracy_metric_key = \"training_accuracy\"\n",
    "training_epoch_key = \"training_iteration\"\n",
    "\n",
    "\n",
    "def trainable(config):\n",
    "    ## SETUP DATASETS\n",
    "\n",
    "    HP_BATCH_SIZE = config[\"HP_BATCH_SIZE\"]\n",
    "\n",
    "    trainset = torchvision.datasets.CIFAR10(\n",
    "        root=dataset_location, train=True, download=False, transform=transform\n",
    "    )\n",
    "    trainloader = DataLoader(\n",
    "        trainset, batch_size=HP_BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True\n",
    "    )\n",
    "\n",
    "    testset = torchvision.datasets.CIFAR10(\n",
    "        root=dataset_location, train=False, download=False, transform=transform\n",
    "    )\n",
    "    testloader = DataLoader(\n",
    "        testset, batch_size=HP_BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True\n",
    "    )\n",
    "\n",
    "    ## SETUP NET\n",
    "\n",
    "    HP_NET_OUT_CHANNELS_1 = config[\"HP_NET_OUT_CHANNELS_1\"]\n",
    "    HP_NET_OUT_CHANNELS_2 = config[\"HP_NET_OUT_CHANNELS_2\"]\n",
    "    HP_NET_KERNEL_SIZE = config[\"HP_NET_KERNEL_SIZE\"]\n",
    "    HP_NET_LINEAR_SIZE_1 = config[\"HP_NET_LINEAR_SIZE_1\"]\n",
    "    HP_NET_LINEAR_SIZE_2 = config[\"HP_NET_LINEAR_SIZE_2\"]\n",
    "\n",
    "    net = Net(\n",
    "        HP_NET_OUT_CHANNELS_1,\n",
    "        HP_NET_OUT_CHANNELS_2,\n",
    "        HP_NET_KERNEL_SIZE,\n",
    "        HP_NET_LINEAR_SIZE_1,\n",
    "        HP_NET_LINEAR_SIZE_2,\n",
    "    )\n",
    "\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            net = nn.DataParallel(net)\n",
    "    net.to(device)\n",
    "\n",
    "    HP_SGD_LEARNING_RATE = config[\"HP_SGD_LEARNING_RATE\"]\n",
    "    HP_SGD_MOMENTUM = config[\"HP_SGD_MOMENTUM\"]\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = optim.SGD(\n",
    "        net.parameters(), lr=HP_SGD_LEARNING_RATE, momentum=HP_SGD_MOMENTUM\n",
    "    )\n",
    "\n",
    "    if train.get_checkpoint():\n",
    "        with train.get_checkpoint().as_directory() as checkpoint_dir:\n",
    "            with open(os.path.join(checkpoint_dir, \"data.pkl\"), \"rb\") as fp:\n",
    "                checkpoint_state = pickle.load(fp)\n",
    "                start_epoch = checkpoint_state[\"epoch\"]\n",
    "                net.load_state_dict(checkpoint_state[\"net_state_dict\"])\n",
    "                optimizer.load_state_dict(checkpoint_state[\"optimizer_state_dict\"])\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "\n",
    "    num_epochs = 100  # for now it is constant to make it shorter\n",
    "\n",
    "    for i in range(start_epoch, num_epochs):\n",
    "        ## ONE TRAINING EPOCH\n",
    "        for data in trainloader:\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs.to(device))\n",
    "            loss = criterion(outputs, labels.to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        ## EVALUATION ON TEST SET AFTER ONE TRAINING EPOCH\n",
    "        preds = []\n",
    "        ground_truth = []\n",
    "        # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "        with torch.no_grad():\n",
    "            for data in testloader:\n",
    "                images, labels = data\n",
    "                # calculate outputs by running images through the network\n",
    "                outputs = net(images)\n",
    "                # the class with the highest energy is what we choose as prediction\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                preds.append(predicted)\n",
    "                ground_truth.append(labels)\n",
    "\n",
    "        preds = torch.hstack(preds)\n",
    "        ground_truth = torch.hstack(ground_truth)\n",
    "        accuracy = accuracy_score(ground_truth, preds)\n",
    "\n",
    "        with tempfile.TemporaryDirectory() as checkpoint_dir:\n",
    "            with open(os.path.join(checkpoint_dir, \"data.pkl\"), \"wb\") as fp:\n",
    "                pickle.dump({\"epoch\": i}, fp)\n",
    "                pickle.dump({\"net_state_dict\": net.state_dict()}, fp)\n",
    "                pickle.dump({\"optimizer_state_dict\": optimizer.state_dict()}, fp)\n",
    "\n",
    "            checkpoint = Checkpoint.from_directory(checkpoint_dir)\n",
    "            train.report(\n",
    "                {training_epoch_key: i, accuracy_metric_key: accuracy},\n",
    "                checkpoint=checkpoint,\n",
    "            )\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zwróćmy uwagę na ostatnią linijkę z `train.report(...)`. Ona ma za zadanie po każdej epoce przekazać do Ray informację o wyniku, by można było ocenić, czy eksperyment idzie w dobrą stronę."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search algorithm\n",
    "\n",
    "Domyślnie jako formę przeszukiwania Ray uruchamia zestaw $n$ losowych konfiguracji hiperparametrów i przyjmuje najlepszy wynik, a więc klasyczny random search. Istnieją jednak bardziej usystematyzowane metody, które Ray dostarcza, np. [NevergradSearch](https://docs.ray.io/en/latest/tune/api/suggestion.html#nevergrad) czy [HyperOptSearch](https://docs.ray.io/en/latest/tune/api/suggestion.html#tune-hyperopt). Bazują na istniejących bibliotekach SOTA do optymalizacji, a Ray opakowuje je w jeden interfejs.\n",
    "\n",
    "W Ray, `search algorithm` nie wpływa na proces eksperymentu w jego trakcie, w szczególności nie przerywa go, gdy źle rokuje. Tym zajmuje się `scheduler`, o którym powiemy sobie za chwilę. `Search algorithm` tylko generuje konfiguracje hiperparametrów do sprawdzenia i wybiera najlepszy wynik.\n",
    "\n",
    "[Więcej algorytmów](https://docs.ray.io/en/latest/tune/api/suggestion.html#tune-search-algorithms-tune-search)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "source": [
    "from ray.tune.search.hyperopt import HyperOptSearch\n",
    "\n",
    "search_algorithm = HyperOptSearch(metric=accuracy_metric_key, mode=\"max\")\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scheduler\n",
    "\n",
    "Super że mamy przeszukiwanie z użyciem optymalizacji bayesowskiej. Ale co nam z tego, skoro proces uczenia tak naprawdę będzie odpalony na każdej konfiguracji, przebiegnie do końca i wybierzemy sobie najwyższy wynik... Czym to się różni od grid/random search? :(\n",
    "\n",
    "Tu z pomocą przychodzi nam `scheduler`, który może wcześniej wyłączyć nieobiecujące eksperymenty, a pozostałe kontynuować.\n",
    "\n",
    "Ray oferuje kilka algorytmów SOTA `scheduler`, np. [ASHA (Async Successive Halving Algorithm)](https://docs.ray.io/en/latest/tune/api/schedulers.html#tune-scheduler-hyperband) czy [HyperBand](https://docs.ray.io/en/latest/tune/api/schedulers.html#tune-original-hyperband)\n",
    "\n",
    "My skorzystamy dziś z ASHA, by móc łatwo zwolnić zasoby dla eksperymentów, w których model nie uczy się efektywnie.\n",
    "\n",
    "[Więcej przykładów schedulerów](https://docs.ray.io/en/latest/tune/api/schedulers.html#tune-trial-schedulers-tune-schedulers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "source": [
    "asha_scheduler = ASHAScheduler(\n",
    "    time_attr=training_epoch_key,\n",
    "    metric=accuracy_metric_key,\n",
    "    mode=\"max\",\n",
    "    max_t=100,\n",
    "    grace_period=10,\n",
    "    reduction_factor=3,\n",
    "    brackets=1,\n",
    ")\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuner\n",
    "\n",
    "Mając to wszystko razem, możemy utworzyć instancję `Tuner`, w której zawrzemy wszystkie przygotowane składowe eksperymentu. Następnie uruchomimy funkcję `fit()` i będziemy mogli sprawdzać wyniki eksperymentów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "source": [
    "%%html\n",
    "<style>\n",
    ".tuneStatus {\n",
    "    background-color: white !important;\n",
    "}\n",
    ".trialStatus {\n",
    "    overflow-x: scroll;\n",
    "}\n",
    "</style>"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "source": [
    "ray.shutdown()\n",
    "ray.init()\n",
    "\n",
    "tuner = tune.Tuner(\n",
    "    trainable=tune.with_resources(\n",
    "        trainable,\n",
    "        resources=ScalingConfig(use_gpu=True),\n",
    "    ),\n",
    "    # replace the line above if you don't have GPU\n",
    "    # trainable=trainable,\n",
    "    tune_config=tune.TuneConfig(\n",
    "        scheduler=asha_scheduler,\n",
    "        search_alg=search_algorithm,\n",
    "        num_samples=10,\n",
    "        # you can use more concurrent trials if your PC can handle them\n",
    "        max_concurrent_trials=4,\n",
    "    ),\n",
    "    param_space=config,\n",
    ")\n",
    "\n",
    "results = tuner.fit()\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ponieważ co odpalenie Ray Tune to inny zestaw wartości hiperparametrów, inne seedy i inne wyniki, dlatego też skopiowałem logi z tego odpalenia treningu (które u mnie trwało ok. 2h, ponieważ odnoszę wrażenie że nie wszystkie próby korzystały z GPU) do folderu `./data/sample-ray-logs`.\n",
    "\n",
    "Logi z każdego treningu Raya są kompatybilne z Tensorboardem, zatem by obejrzeć rezultaty wszystkich prób, możemy uruchomić `tensorboard --logdir ./data/sample-ray-logs` i obejrzeć wyniki.\n",
    "\n",
    "Możemy zauważyć korzystny zestaw hiperparametrów (skok z $50\\%$ accuracy do $\\sim 68\\%$):\n",
    "\n",
    "| HP_BATCH_SIZE | HP_NET_KERNEL_SIZE | HP_NET_LINEAR_SIZE_1 | HP_NET_LINEAR_SIZE_2 | HP_NET_OUT_CHANNELS_1 | HP_NET_OUT_CHANNELS_2 | HP_SGD_LEARNING_RATE | HP_SGD_MOMENTUM       | ray/tune/training_accuracy |\n",
    "|---------------|--------------------|----------------------|----------------------|-----------------------|-----------------------|----------------------|-----------------------|----------------------------|\n",
    "| 128.0         | 3.0                | 9.0                  | 28.0                 | 54.0                  | 61.0                  | 0.02927921466663529  | 8.030329827594446e-05 | 0.6794000267982483         |\n",
    "\n",
    "Spróbujmy wyuczyć model na nim jeszcze raz, zobaczmy co się stanie..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "source": [
    "net = Net(\n",
    "    54,\n",
    "    61,\n",
    "    3,\n",
    "    9,\n",
    "    18,\n",
    ").cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = optim.SGD(\n",
    "    net.parameters(), lr=0.02927921466663529, momentum=8.030329827594446e-05\n",
    ")\n",
    "\n",
    "trainloader = DataLoader(\n",
    "    trainset, batch_size=128, shuffle=True, num_workers=2, pin_memory=True\n",
    ")\n",
    "\n",
    "for epoch in range(100):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs.to(device))\n",
    "        loss = criterion(outputs, labels.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:  # print every 2000 mini-batches\n",
    "            print(f\"[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 100:.3f}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "print(\"Finished Training\")\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "source": [
    "preds = []\n",
    "ground_truth = []\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(images.cuda())\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        preds.append(predicted)\n",
    "        ground_truth.append(labels)\n",
    "\n",
    "\n",
    "preds = torch.hstack(preds)\n",
    "ground_truth = torch.hstack(ground_truth)\n",
    "\n",
    "print(\n",
    "    f\"Accuracy of the network on the 10000 test images: {accuracy_score(ground_truth.cpu(), preds.cpu())} %\"\n",
    ")\n",
    "print(\n",
    "    f\"F1 macro of the network on the 10000 test images: {f1_score(ground_truth.cpu(), preds.cpu(), average='macro')} %\"\n",
    ")\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wynik jest bardzo podobny do tego, który uzyskaliśmy poprzednim razem (różnica $2$ p.p.). To znaczy, że:\n",
    "\n",
    "- udało się nam poprawić za pomocą Raya wynik modelu\n",
    "- proponowane przez Raya hiperparametry dają podobne wyniki również po całym procesie przeszukiwania, są zatem wiarygodne\n",
    "\n",
    "Cel zadania postawiony w tym notebooku został osiągnięty."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
